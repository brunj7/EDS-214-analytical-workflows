[
  {
    "objectID": "day4-documenting.html",
    "href": "day4-documenting.html",
    "title": "Documenting things",
    "section": "",
    "text": "Only a few people with free time ahead of them will sit wondering about what to do next and think “what if I were to write some documentation!?”. Make it part of your workflow, and do not let it get out of sync too much as you iterate on your analysis.\nhttps://twitter.com/JenMsft/status/1557218211971489792"
  },
  {
    "objectID": "day4-documenting.html#the-power-of-readme",
    "href": "day4-documenting.html#the-power-of-readme",
    "title": "Documenting things",
    "section": "The power of README",
    "text": "The power of README\nREADME files are not a new thing. They have been around computer projects since the early days. One great thing about the popularization of supporting the markdown syntax (and its web rendering in most code repositories) is that you can move beyond a simple text file and start to present a compelling entry point to your project that can link to various parts and external resources.\nGood types of information to have on a README:\n\nTitle capturing the essence of the project\nList of current contributors\nA short explanation of the goal / purpose\nHow to install / where to start\nA quick demo on how to use the content (can be a link to another document as well)\nWhat to do if a bug is spotted\nHow to contribute\nLicensing\nAcknowledgements of authors, contributors, sponsors or other related work\n\nAdding images, short videos / animations can make a README more engaging.\n\nData README\nMost data repositories will ask you to provide some kind of README file to help describe the content you are archiving. Here is a template you may customize for your project needs: https://doi.org/10.5281/zenodo.10828379\n\n\nCode README\nNeed some inspiration ?\n\nHere is a interesting template: https://github.com/navendu-pottekkat/awesome-readme/tree/master\nWhen you start an R package with the usethis package, a README will be created for you with all the relevant sections for such type of project.\npick a package you like and inspect their README"
  },
  {
    "objectID": "day4-documenting.html#making-your-code-readable",
    "href": "day4-documenting.html#making-your-code-readable",
    "title": "Documenting things",
    "section": "Making your code readable",
    "text": "Making your code readable\n\n\n\n\n\nhttps://twitter.com/cjm4189/status/1557346489613094914\n\n\n\n\nIt is important to make your code easy to read if you hope that others will reuse it. It starts with using a consistent style withing your scripts (at least within a project).\n\nHere is a good style guide for R: https://style.tidyverse.org/\nStyle guide for Python: https://www.python.org/dev/peps/pep-0008/\n\n\nimport this\n\nThe Zen of Python, by Tim Peters\n\nBeautiful is better than ugly.\nExplicit is better than implicit.\nSimple is better than complex.\nComplex is better than complicated.\nFlat is better than nested.\nSparse is better than dense.\nReadability counts.\nSpecial cases aren't special enough to break the rules.\nAlthough practicality beats purity.\nErrors should never pass silently.\nUnless explicitly silenced.\nIn the face of ambiguity, refuse the temptation to guess.\nThere should be one-- and preferably only one --obvious way to do it.\nAlthough that way may not be obvious at first unless you're Dutch.\nNow is better than never.\nAlthough never is often better than *right* now.\nIf the implementation is hard to explain, it's a bad idea.\nIf the implementation is easy to explain, it may be a good idea.\nNamespaces are one honking great idea -- let's do more of those!\n\n\nThere is also the visual aspect of the code that should not be neglected. Like a prose, if you receive a long text without any paragraphs, you might be not very excited about reading it. Indentation, spaces, and empty lines should be leveraged to make a script visually inviting and easy to read. The good news is that most of the Integrated Development Environment (IDE) will help you to do so by auto formatting your scripts according to conventions. Note that also a lot of IDEs, such as RStudio, rely on some conventions to ease the navigation of scripts and notebooks. For example, try to add four - or # after a line starting with one or several # in an R Script!"
  },
  {
    "objectID": "day4-documenting.html#comments",
    "href": "day4-documenting.html#comments",
    "title": "Documenting things",
    "section": "Comments",
    "text": "Comments\nReal Programmers don’t comment their code. If it was hard to write, it should be hard to understand.\nTom Van Vleck, based on people he knew…_ (https://multicians.org/thvv/realprogs.html)\nJoke aside, it is really hard to comment too much your code, because even steps that might seem trivial today might not be so anymore in a few weeks or months for now. In addition, a well commented code is more likely to be read by others. Note also that comments should work in complement of the code and should not being seen as work around vague naming conventions of variables or functions.\n\nx &lt;- 9.81  #  gravitational acceleration\n\ngravity_acc &lt;- 9.81  #  gravitational acceleration\n\n\nHeader\nIt is good to add a header to your script that will provide basic information such as:\n\nPurpose of the script (Long title style)\nWho are the authors\nA contact email\n\nOptional:\n\nA longer description about the script purpose\nA starting date and potentially last updated one, although this information becomes redundant with repository information\n\nNote that R Studio does something similar by default when creating an new R Markdown document!\n\n\nInline\nIt does not matter if you are using a script or notebook. It is important to provide comments along your code to complement it by:\n\nexplaining what the code does\ncapturing decisions that were made on the analytical side. For example, why a specific value was used for a threshold.\nspecifying when some code was added to handle an edge case such as an unexpected value in the data (so a new user doesn’t have to guess what does lines of code and might want to delete them assuming it is not necessary)\n\nOther thoughts:\n\nIt is OK to state (what seems) the obvious (some might disagree with this statement)\nTry to keep comments to the point and short\n\n\n\nFunctions\nBoth Python and R have conventions on how to document functions. Adopting those conventions will help you to make your code readable but also to automate part of the documentation development.\n\nRoxygen2\nThe goal of roxygen2 is to make documenting your code as easy as possible. It can dynamically inspect the objects that it’s documenting, so it can automatically add data that you’d otherwise have to write by hand.\nHow do we insert it? Make sure you cursor is inside the function you want to document and from RStudio Menu Code -&gt; Insert Roxygen Skeleton\nExample:\n\n#' Add together two numbers\n#'\n#' @param x A number\n#' @param y A number\n#' @return The sum of \\code{x} and \\code{y}\n#' @examples\n#' add(1, 1)\n#' add(10, 1)\nadd2 &lt;- function(x, y) {\n  x + y\n}\n\nTry it! - Copy the function (without the documentation) in a new script - Add a third parameter to the function such as it sums 3 numbers - Add the Roxygen skeleton - Fill it to best describe your function\nNote that when you are developing an R package, the Roxygen skeleton can be leveraged to develop the help pages of your package so you only have one place to update and the help will synchronize automatically.\n\n\nPython Docstring\nA docstring is a string literal that occurs as the first statement in a module, function, class, or method definition. Such a docstring becomes the __doc__ special attribute of that object.\n\ndef complex(real=0.0, imag=0.0):\n    \"\"\"Form a complex number.\n\n    Keyword arguments:\n    real -- the real part (default 0.0)\n    imag -- the imaginary part (default 0.0)\n    \"\"\"\n    if imag == 0.0 and real == 0.0:\n        return complex_zero\n\nHere for more: https://www.python.org/dev/peps/pep-0257/"
  },
  {
    "objectID": "day4-documenting.html#leveraging-notebooks",
    "href": "day4-documenting.html#leveraging-notebooks",
    "title": "Documenting things",
    "section": "Leveraging Notebooks",
    "text": "Leveraging Notebooks\nAs we have discussed and experimented with Notebooks during the week. It is because Notebooks provide space to further develop content, such as methodology, around the code you are developing in your analysis. Notebooks also enable you to integrate the outputs of your scientific research with the code that was used to produce it. Finally, notebooks can be rendered into various format that let them share with a broad audience.\nNotebooks are not only used within the scientific community, see here for some thoughts from Airbnb data science team."
  },
  {
    "objectID": "day4-documenting.html#hands-on",
    "href": "day4-documenting.html#hands-on",
    "title": "Documenting things",
    "section": "Hands-on",
    "text": "Hands-on\n\nDocumenting\n\ngetPercent &lt;- function( value, pct ) {\n    result &lt;- value * ( pct / 100 )\n    return( result )\n}\n\nTry adding the Roxygen Skeleton to this function and fill all the information you think is necessary to document the function\n\n\nCommenting\nLet’s try to improve the readability and documentation of this repository: https://github.com/brunj7/better-comments. Follow the instructions on the README\nFor inspiration, you can check out the NASA code for APOLLO 11 dating from 1969: https://github.com/chrislgarry/Apollo-11!!"
  },
  {
    "objectID": "day4-documenting.html#computing-environment",
    "href": "day4-documenting.html#computing-environment",
    "title": "Documenting things",
    "section": "Computing Environment",
    "text": "Computing Environment\nYour analysis was done with specific versions of the program used (e.g. R 4.4.3) but also of all the packages involved, as well as the specifications of the Operating System (OS) that was used. The good use is that there are tools to let you systematically capture this information.\nIn R:\nsessionInfo() or devtools::session_info() are great ways to capture all this information. You should save it into a session_info.txt file to include in your GitHub repository\n\nVirtual environments\nYou can even go a step further and help others to recreate the same computing environment that you used independently of what versions you have installed on your machine: https://rstudio.github.io/renv/articles/renv.html"
  },
  {
    "objectID": "day4-documenting.html#metadata",
    "href": "day4-documenting.html#metadata",
    "title": "Documenting things",
    "section": "Metadata",
    "text": "Metadata\nThis a very important topic for scientific reproducibility and for today we will be only provide a partial overview of this broader topic.\n\n\n\n\n\nData life cycle, DataONE\n\n\n\n\nMetadata (data about data) is an important part of the data life cycle because it enables data reuse long after the original collection. The goal is to have enough information for the researcher to understand the data, interpret the data, and then re-use the data in another study.\n\n\n\n\n\nmetadata analogy\n\n\n\n\nHere are good questions to answer with your metadata:\n\nWhat was measured?\nWho measured it?\nWhen was it measured?\nWhere was it measured?\nHow was it measured?\nHow is the data structured?\nWhy was the data collected?\nWho should get credit for this data (researcher AND funding agency)?\nHow can this data be reused (licensing)?\n\n\nMetadata standards\nHow do you organize all this information? You could use a free-form format, like a README file or spreadsheet. But there is also a great advantage to using a more standardized way that will make the content not only Human readable but also machine-readable. This will enhance the data discovery as specific information will be potentially tagged or attributed to specific aspects of your data (e.g. spatial or temporal coverage, taxonomy, …).\nThere are a number of environmental metadata standards (think, templates) that you could use, including the Ecological Metadata Language (EML), Geospatial Metadata Standards like ISO 19115 and ISO 19139, the Biological Data Profile (BDP), Dublin Core, Darwin Core, PREMIS, the Metadata Encoding and Transmission Standard (METS), and the list goes on and on.\nSome repositories will have standards baked into their systems, so if you plan to archive and preserve your data in a disciplinary repository with specific metadata requirements it is always important to check what are these in advance. You may need to rework your existing documentation accordingly."
  },
  {
    "objectID": "day4-documenting.html#data-provenance-semantics",
    "href": "day4-documenting.html#data-provenance-semantics",
    "title": "Documenting things",
    "section": "Data provenance & semantics",
    "text": "Data provenance & semantics\nData provenance refers to one’s ability to trace the original source of a data set to the raw data that were used as input for the processing/analysis that led to the creation of this data set. It can be done more or less formally and this is an active area of research. Today, we will be focusing on capturing the information about the data you are collecting. Here are a set of good questions to help you in that process:\n\nSource / owner (Person, institution, website, ….)\nWhen was it acquired ?\nBy whom on the WG ?\nWhere is it currently located (Google drive, server, ….) ?\nShort description of the data\nTrack if it is used in your analysis\n\nHere is a template of a data log that could hep to store this information\nAnother important and related aspect and also active field of research is data semantics. Often data sets store complex information and concepts that can be described more or less accurately. Let’s take an example, you have received a csv file storing a table with several variables about a fish stock assessment. One of the variables is named “length”. However, there are many ways to measure the length of a fish. Which one is it?\n\n\n\n\n\n\n\n\n\nData semantics aims at clearly identify those concepts relying on vocabularies and ontologies, such as ENVO in environmental sciences. In addition, it enables the leverage relations between those concepts to help with (data) discovery."
  },
  {
    "objectID": "day4-documenting.html#yourself",
    "href": "day4-documenting.html#yourself",
    "title": "Documenting things",
    "section": "Yourself",
    "text": "Yourself\nYou as an Author!!\nIt is important to be able to reference yourself as a researcher and as an author of your work in a non ambiguous manner. ORCID is a great way to create a persistent digital identifier (an ORCID iD) that you own and control, and that distinguishes you from every other researcher. ORCID is also more and more used as an authentication system for many services (e.g. data repositories)."
  },
  {
    "objectID": "day4-documenting.html#licensing",
    "href": "day4-documenting.html#licensing",
    "title": "Documenting things",
    "section": "Licensing",
    "text": "Licensing\nIt is a good practice to add a license to a repository / project. It will help to clarify what are the expectations regarding using and potentially contributing to this work.\nHere is a good website to choose a license:\nHere is also good set of instructions on how to make this happen on a GitHub repository: https://docs.github.com/en/github/creating-cloning-and-archiving-repositories/creating-a-repository-on-github/licensing-a-repository\nNote that for content (such as this course), there is also another type of licensing that can be used: https://creativecommons.org/licenses/"
  },
  {
    "objectID": "day4-documenting.html#further-reading",
    "href": "day4-documenting.html#further-reading",
    "title": "Documenting things",
    "section": "Further Reading",
    "text": "Further Reading\n\nAwesome README: https://towardsdatascience.com/how-to-write-an-awesome-readme-68bf4be91f8b\nHow to write a great README: https://x-team.com/blog/how-to-write-a-great-readme/\nPython Hichhiker’s guide:\n\nstyle: https://docs.python-guide.org/writing/style/\ndocumentation: https://docs.python-guide.org/writing/documentation/\n\nDocumenting Python Code: A Complete Guide: https://realpython.com/documenting-python-code/\nRoxygen2: https://cran.r-project.org/web/packages/roxygen2/vignettes/roxygen2.html\nIntroduction to Software Engineering by Jason Coposky: https://github.com/NCEAS/training/blob/master/2014-oss/day-09/IntroductionToSoftwareClass.pdf\nFunctional programming in R: http://adv-r.had.co.nz/Functional-programming.html#functional-programming\nScoping in R: http://adv-r.had.co.nz/Functions.html\nhttps://www.quora.com/What-is-the-difference-between-programming-languages-markup-languages-and-scripting-languages\nhttp://stackoverflow.com/questions/17253545/scripting-language-vs-programming-language\nSoftware carpentry: https://swcarpentry.github.io/r-novice-inflammation/06-best-practices-R/"
  },
  {
    "objectID": "day4-documenting.html#acknowledgements",
    "href": "day4-documenting.html#acknowledgements",
    "title": "Documenting things",
    "section": "Acknowledgements",
    "text": "Acknowledgements\nLarge portion of this material have been adapted from NCEAS Reproducible Research Techniques for Synthesis course https://learning.nceas.ucsb.edu/2021-02-RRCourse/"
  },
  {
    "objectID": "day2-rstudio_server.html",
    "href": "day2-rstudio_server.html",
    "title": "RStudio server",
    "section": "",
    "text": "From an user perspective, RStudio Server is your familiar RStudio interface in your web browser. The big difference however is that with RStudio Server the computation will be running on the remote machine instead of your local personal computer. This also means that the files you are seeing through the RStudio Server interface are located on the remote machine. And this also include your R packages!!! This remote file management is the main change you will have to adopt in your workflow.\nTo help with remote files management, the RStudio Server interface as few additional features that we will be discussing in the following sections.\n\n\n\n\n\n\n\n\n\n\n\n\nGot to: https://taylor.bren.ucsb.edu/\nEnter your credentials\nYou are in!\n\n\n\n\n\n\n\n\n\n\n\nClick on the New Session button. You can see that you are able to start both an R (Studio) and jupyter notebook session. Let’s take a few minutes to experiment with the different options.\n\nFor this session, we are going to select the RStudio option and hit Start Session.\n\n\n\n\n\n\n\n\n\nYou should now see a very familiar interface :) Except it is running on the server with a lot of resources at your fingertips!!\n\n\nLet’s explore explore a little bit the file structure on the server. By default on a Linux server, you are located in the home folder. This folder is only accessible to you and it is where you can store your personal files on a server. You should see 2 folders: R and H\n\n\n\n\n\n\n\n\n\nThe R folder is where your local R packages will be installed, you can ignore it. The H is your H drive that the Bren School is offering to all its students. If you click on it you should see any files you have uploaded there.\nLet us make a folder named github by click on the New Folder button at the top of the tab. We will use this folder (also named directory in linux/unix terms) to clone any GitHub repository.\n\n\n\nIf we go to the Packages tab, we can see a long list of packages that have already be installed by our system administrator (Brad). Those packages have been installed server wide, meaning that all the users have access to them.\n\n\n\n\n\n\n\n\n\nA user can also installed her/his own packages. Let’s try to install the remote package that lets you install R packages directly from GitHub: install.packages(\"remotes\"). Once done, note a new section that appeared on the Packages tab named User Library. Each of us have now its own copy of the package installed (in this R folder we were talking about a few minutes ago).\n\n\n\n\n\n\n\n\n\nA few notes:\n\nIn this example we will have made a better choice to have the remotes package installed once at the system level\nSome R packages depend on external libraries that need to be installed on the server. Those libraries will have to be installed by the system administrator first before you can install the R package\nInstalling an R package on a linux machine generally requires compilation of the code and will thus take more time to install than when you install it from pre-compiled binaries\n\nLook now inside you R folder!!"
  },
  {
    "objectID": "day2-rstudio_server.html#connecting-to-meds-analytical-server",
    "href": "day2-rstudio_server.html#connecting-to-meds-analytical-server",
    "title": "RStudio server",
    "section": "",
    "text": "Got to: https://taylor.bren.ucsb.edu/\nEnter your credentials\nYou are in!\n\n\n\n\n\n\n\n\n\n\n\nClick on the New Session button. You can see that you are able to start both an R (Studio) and jupyter notebook session. Let’s take a few minutes to experiment with the different options.\n\nFor this session, we are going to select the RStudio option and hit Start Session.\n\n\n\n\n\n\n\n\n\nYou should now see a very familiar interface :) Except it is running on the server with a lot of resources at your fingertips!!\n\n\nLet’s explore explore a little bit the file structure on the server. By default on a Linux server, you are located in the home folder. This folder is only accessible to you and it is where you can store your personal files on a server. You should see 2 folders: R and H\n\n\n\n\n\n\n\n\n\nThe R folder is where your local R packages will be installed, you can ignore it. The H is your H drive that the Bren School is offering to all its students. If you click on it you should see any files you have uploaded there.\nLet us make a folder named github by click on the New Folder button at the top of the tab. We will use this folder (also named directory in linux/unix terms) to clone any GitHub repository.\n\n\n\nIf we go to the Packages tab, we can see a long list of packages that have already be installed by our system administrator (Brad). Those packages have been installed server wide, meaning that all the users have access to them.\n\n\n\n\n\n\n\n\n\nA user can also installed her/his own packages. Let’s try to install the remote package that lets you install R packages directly from GitHub: install.packages(\"remotes\"). Once done, note a new section that appeared on the Packages tab named User Library. Each of us have now its own copy of the package installed (in this R folder we were talking about a few minutes ago).\n\n\n\n\n\n\n\n\n\nA few notes:\n\nIn this example we will have made a better choice to have the remotes package installed once at the system level\nSome R packages depend on external libraries that need to be installed on the server. Those libraries will have to be installed by the system administrator first before you can install the R package\nInstalling an R package on a linux machine generally requires compilation of the code and will thus take more time to install than when you install it from pre-compiled binaries\n\nLook now inside you R folder!!"
  },
  {
    "objectID": "day1-github_branches.html",
    "href": "day1-github_branches.html",
    "title": "Collaborative Coding Workflows: Branching",
    "section": "",
    "text": "When you collaborate closely and actively with colleagues, you do not want necessarily to have to review all their changes through pull requests. You can then give them write access (git push) to your repository to allow them to directly edit and contribute to its content. This is the workflow we will recommend to use within your working group.\n\n\n\nClick on the repository\nOn the top tabs, click \nOn the left pane, click Manage access and click on “Invite a Collaborator” to enter the usernames you want to add\n\n\n\n\ncollaborators\n\n\nUnder this collaborative workflow, we recommend to use git branches combined with pull requests to avoid conflicts and to track and discuss collaborators contributions."
  },
  {
    "objectID": "day1-github_branches.html#collaborating-through-write-push-access",
    "href": "day1-github_branches.html#collaborating-through-write-push-access",
    "title": "Collaborative Coding Workflows: Branching",
    "section": "",
    "text": "When you collaborate closely and actively with colleagues, you do not want necessarily to have to review all their changes through pull requests. You can then give them write access (git push) to your repository to allow them to directly edit and contribute to its content. This is the workflow we will recommend to use within your working group.\n\n\n\nClick on the repository\nOn the top tabs, click \nOn the left pane, click Manage access and click on “Invite a Collaborator” to enter the usernames you want to add\n\n\n\n\ncollaborators\n\n\nUnder this collaborative workflow, we recommend to use git branches combined with pull requests to avoid conflicts and to track and discuss collaborators contributions."
  },
  {
    "objectID": "day1-github_branches.html#working-with-branches",
    "href": "day1-github_branches.html#working-with-branches",
    "title": "Collaborative Coding Workflows: Branching",
    "section": "Working with branches",
    "text": "Working with branches\n\nCreating a new branch\nIn RStudio, you can create a branch using the git tab.\n\nClick on the branch button\n\n\n\n\n\n\n\n\n\n\n\nFill the branch name in the new branch window; in this example, we are going to use test for the name; leave the other options as default and click create\n\n\n\n\n\n\n\n\n\n\n\nyou will be directly creating a local and remote branch and switch to it\n\n\n\n\n\n\n\n\n\n\nCongratulations you just created your first branch!\nLet us check on Github:\n\n\n\n\n\n\n\n\n\nAs you can see, now there are two branches on our remote repository: - main - test\n\n\nUsing a branch\nHere there is nothing new. The workflow is exactly the same as we did before, except our commits will be created on the test branch instead of the main branch."
  },
  {
    "objectID": "day1-github_branches.html#hands-on",
    "href": "day1-github_branches.html#hands-on",
    "title": "Collaborative Coding Workflows: Branching",
    "section": "Hands-on",
    "text": "Hands-on\nLet’s practice this by team of 2!!\nhttps://github.com/brunj7/eds214-handson-ghcollab"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "section": "",
    "text": "Collaborative and Reproducible Environmental Data Science as summarized by the MEDS Students cohort 21-22, art by Allison Horst"
  },
  {
    "objectID": "index.html#course-description",
    "href": "index.html#course-description",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "section": "Course description",
    "text": "Course description\nThe generation and analysis of environmental data is often a complex, multi-step process that may involve the collaboration of many people. Increasingly tools that document and help to organize workflows are being used to ensure reproducibility, shareability, and transparency of the results. This course will introduce students to the conceptual organization of workflows (including code, documents, and data) as a way to conduct reproducible analyses. These concepts will be combined with the practice of various software tools and collaborative coding techniques to develop and manage multi-step analytical workflows as a team.\n\nWhy going reproducible ?\nThere are many reasons why it is essential to make your science reproducible and how the necessity of openness is a cornerstone of the integrity and efficacy of the scientific research process. Here we will also be focusing on why making your work reproducible will empower you to iterate quickly, integrate new information more easily to iterate quickly, scale your analysis to larger data sets, and better collaborate by receiving feedback and contributions from others, as well as enable your “future self” to reuse and build from your own work.\nTo make your data-riven research reproducible, it is important to develop scientific workflows that will be relying on programming to accomplish the necessary tasks to go from the raw data to the results (figures, new data, publications, …) of your analysis. Scripting languages, even better open ones, such as R and python, are well-suited for scientists to develop reproducible scientific workflows. Those scripting languages provide a large ecosystem of libraries (also referred to as packages or modules) that are ready to be leveraged to conduct analysis and modeling. In this course we will introduce how to use R, git and GitHub to develop such workflows as a team.\n\n\n\n\n\nWorkflow example using the tidyverse. Note the program box around the workflow and the iterative nature of the analytical process described. Source: R for Data Science https://r4ds.had.co.nz/\n\n\n\n\nTwo points to stress about this figure:\n\nWorkflows are rarely linear… even less so their implementation\nNote the programming box – yes, you’ll need to code this :)\n\nWorkflows are developed iteratively, and one of the most helpful things you can do as a data scientist is to talk about them with your research team."
  },
  {
    "objectID": "index.html#teaching-team",
    "href": "index.html#teaching-team",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "section": "Teaching team",
    "text": "Teaching team\n\nJulien Brun, Instructor\nBrian Lee, TA\n\nMEDS Slack is the best way to communicate with us."
  },
  {
    "objectID": "index.html#important-links",
    "href": "index.html#important-links",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "section": "Important links",
    "text": "Important links\n\nCourse syllabus\nCode of Conduct"
  },
  {
    "objectID": "index.html#predictable-daily-schedule",
    "href": "index.html#predictable-daily-schedule",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "section": "Predictable daily schedule",
    "text": "Predictable daily schedule\nCourse dates: Monday (2024-08-26) - Friday (2024-08-30)\nEDS 214 is an intensive 1-week long graded 2-unit course. Students should plan to attend all scheduled sessions. All course requirements will be completed between 10am and 5pm PST (M - F), i.e. you are not expected to do additional work for EDS 214 outside of those hours, unless you are working with the Teaching Assistant in student hours.\n\nDaily schedule (subject to change):\n\n\n\n\n\n\n\nTime (PST)\nActivity\n\n\n\n\n10:00am - 10:50am\nLecture (50 min)\n\n\n10:50am - 11:00am\nBreak (10 min)\n\n\n11:00am - 12:30am\nInteractive Session 1 (90 min)\n\n\n12:30am - 1:30pm\nLunch (60 min)\n\n\n1:30pm - 2:00pm\nLecture (30 min)\n\n\n2:00pm - 3:00pm\nInteractive Session 2 (60 min)\n\n\n3:00pm - 3:10pm\nBreak (10 min)\n\n\n3:10pm - 4:00pm\nFlex time + Q&A (50 min)"
  },
  {
    "objectID": "index.html#learning-objectives",
    "href": "index.html#learning-objectives",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "section": "Learning objectives",
    "text": "Learning objectives\nThe goal of EDS 214 - Analytical Workflows and Scientific Reproducibility is to expose incoming MEDS students to “good enough” practices of scientific programming develop skills in environmental data science to produce reproducible research. By the end of the course, students should be able to:\n\nDevelop knowledge in scientific analytical workflows To learn how to make your data-riven research reproducible, it is important to develop scientific workflows that will be relying on programming to accomplish the necessary tasks to go from the raw data to the results of your analysis (figures, new data, publications, …). Scripting languages, even better open ones such as R and python, are well-suited for scientists to develop reproducible scientific workflows, but are not the only tools you will need to develop reproducible and collaborative workflows\nLearn how to code in a collaborative manner by practicing techniques such as code review and pair programming. Become comfortable asking for and conducting code review using git and GitHub to create pull request, ask feedback from peers, and merge changes into the main repository. Practice pair programming to cement the collaborative development of reproducible analytical workflows\nPractice documenting code and data in a systematic way that will enable your collaborators, including your future self, to understand and reuse your work"
  },
  {
    "objectID": "index.html#grading",
    "href": "index.html#grading",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "section": "Grading",
    "text": "Grading\nThe grading for this course is organized as follow:\n\n50% Class participation\n50% Group project"
  },
  {
    "objectID": "index.html#sessions-subject-to-change",
    "href": "index.html#sessions-subject-to-change",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "section": "Sessions (subject to change)",
    "text": "Sessions (subject to change)\n\n\n\nDay / Session\nTopics\nInteractive Sessions\n\n\n\n\nMonday morning\nReproducible workflows\nPlanning things: from flow charts to pseudocode\n\n\nMonday afternoon\nCoding together\nCollaborating with GitHub; Github collaboration Hands-on; Bonus: Github conflicts\n\n\nTuesday morning\nWorking on a remote server\nthe command line and friends\n\n\nTuesday afternoon\nAutomating things with bash\nbash scripting & uploading things\n\n\nWednesday morning\nHow to get data using APIs\nAPIs hands-on: USGS dataretrieval & metajam\n\n\nWednesday afternoon\nIntroduction to group project\nGroup project\n\n\nThursday morning\nReproducible tools\ntargets & rrtools as examples\n\n\nThursday afternoon\nDocumenting things\nGroup project\n\n\nFriday morning\nSharing things\nGroup project\n\n\nFriday afternoon\nProject presentations\nProject presentations"
  },
  {
    "objectID": "index.html#course-requirements",
    "href": "index.html#course-requirements",
    "title": "EDS 214: Analytical Workflows and Scientific Reproducibility",
    "section": "Course requirements",
    "text": "Course requirements\n\nComputing\n\nMinimum MEDS device requirements\nHave a ready to be used GitHub Account (https://github.com/){target=“_blank”}\nMEDS server Taylor\n\n\n\nTextbook\n\nR for Data Science, 2nd edition: https://r4ds.hadley.nz/\nThe Practice of Reproducible Research: http://www.practicereproducibleresearch.org/"
  },
  {
    "objectID": "day1-git_github_recap.html",
    "href": "day1-git_github_recap.html",
    "title": "git and GitHub recap",
    "section": "",
    "text": "Aka – Say goodbye to script_JB_03v5b.R !!\n\n\n\n\n\n\n\n\n\n\n\nEvery file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control.\nVersion control systems help you track all of the changes to your files, without the spaghetti mess that ensues from simple file renaming. In other words, version control is a system that helps you to manage the different versions of your files in an organized manner. It will help you to never have to duplicate files using save as as a way to keep different versions of a file (see below). Version control help you to create a timeline of snapshots containing the different versions of a file. At any point in time, you will be able to roll back to a specific version. Bonus: you can add a short description (commit message) to remember what each specific version is about.\nWhat is the difference between git and GitHub?\n\ngit: is a version control software used to track files in a folder (a repository)\n\ngit creates a timeline or history of your files\n\nGitHub: is a code repository in the cloud that enables users to store their git repositories and share them with others. Github also add many features to manage projects and document your work.\n\n\n\n\n\nThis section focuses on the code versioning system called Git. Note that there are others, such as Mercurial or svn for example.\nGit is a free and open source distributed version control system. It has many functionalities and was originally geared towards software development and production environment. In fact, Git was initially designed and developed in 2005 by Linux kernel developers (including Linus Torvalds) to track the development of the Linux kernel. Here is a fun video of Linus Torvalds touting Git to Google.\nHow does it work?\nGit can be enabled on a specific folder/directory on your file system to version files within that directory (including sub-directories). In git (and other version control systems) terms, this “tracked folder” is called a repository (which formally is a specific data structure storing versioning information).\nWhat git is not:\n\nGit is not a backup per se\nGit is not good at versioning large files (there are workarounds) =&gt; not meant for data\n\nWhat we want to avoid:\n\n\n\n\n\nhttps://xkcd.com/1597/\n\n\n\n\n\n\nGit can be enabled on a specific folder/directory on your file system to version files within that directory (including sub-directories). In git (and other version control systems) terms, this “tracked folder” is called a repository (which formally is a specific data structure storing versioning information).\nAlthough there many ways to start a new repository, GitHub (or any other cloud solutions, such as GitLab) provide among the most convenient way of starting a repository.\n\n\n\n\n\nGitHub is a company that hosts git repositories online and provides several collaboration features (among which forking). GitHub fosters a great user community and has built a nice web interface to git, also adding great visualization/rendering capacities of your data.\n\nGitHub.com: https://github.com\nA user account: https://github.com/brunj7\nAn organization account: https://github.com/nceas\nNCEAS GitHub instance: https://github.nceas.ucsb.edu/\n\n\n\nThis screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes.\n\nIf we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson and seananderson were fixing things in June and July:\n\nAnd finally, if we drill into the changes made on June 13, we can see exactly what was changed in each file:\n Tracking these changes, and seeing how they relate to released versions of software and files is exactly what Git and GitHub are good for. We will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow."
  },
  {
    "objectID": "day1-git_github_recap.html#the-problem-with-save_as",
    "href": "day1-git_github_recap.html#the-problem-with-save_as",
    "title": "git and GitHub recap",
    "section": "",
    "text": "Every file in the scientific process changes. Manuscripts are edited. Figures get revised. Code gets fixed when problems are discovered. Data files get combined together, then errors are fixed, and then they are split and combined again. In the course of a single analysis, one can expect thousands of changes to files. And yet, all we use to track this are simplistic filenames. You might think there is a better way, and you’d be right: version control.\nVersion control systems help you track all of the changes to your files, without the spaghetti mess that ensues from simple file renaming. In other words, version control is a system that helps you to manage the different versions of your files in an organized manner. It will help you to never have to duplicate files using save as as a way to keep different versions of a file (see below). Version control help you to create a timeline of snapshots containing the different versions of a file. At any point in time, you will be able to roll back to a specific version. Bonus: you can add a short description (commit message) to remember what each specific version is about.\nWhat is the difference between git and GitHub?\n\ngit: is a version control software used to track files in a folder (a repository)\n\ngit creates a timeline or history of your files\n\nGitHub: is a code repository in the cloud that enables users to store their git repositories and share them with others. Github also add many features to manage projects and document your work."
  },
  {
    "objectID": "day1-git_github_recap.html#git",
    "href": "day1-git_github_recap.html#git",
    "title": "git and GitHub recap",
    "section": "",
    "text": "This section focuses on the code versioning system called Git. Note that there are others, such as Mercurial or svn for example.\nGit is a free and open source distributed version control system. It has many functionalities and was originally geared towards software development and production environment. In fact, Git was initially designed and developed in 2005 by Linux kernel developers (including Linus Torvalds) to track the development of the Linux kernel. Here is a fun video of Linus Torvalds touting Git to Google.\nHow does it work?\nGit can be enabled on a specific folder/directory on your file system to version files within that directory (including sub-directories). In git (and other version control systems) terms, this “tracked folder” is called a repository (which formally is a specific data structure storing versioning information).\nWhat git is not:\n\nGit is not a backup per se\nGit is not good at versioning large files (there are workarounds) =&gt; not meant for data\n\nWhat we want to avoid:\n\n\n\n\n\nhttps://xkcd.com/1597/\n\n\n\n\n\n\nGit can be enabled on a specific folder/directory on your file system to version files within that directory (including sub-directories). In git (and other version control systems) terms, this “tracked folder” is called a repository (which formally is a specific data structure storing versioning information).\nAlthough there many ways to start a new repository, GitHub (or any other cloud solutions, such as GitLab) provide among the most convenient way of starting a repository."
  },
  {
    "objectID": "day1-git_github_recap.html#github",
    "href": "day1-git_github_recap.html#github",
    "title": "git and GitHub recap",
    "section": "",
    "text": "GitHub is a company that hosts git repositories online and provides several collaboration features (among which forking). GitHub fosters a great user community and has built a nice web interface to git, also adding great visualization/rendering capacities of your data.\n\nGitHub.com: https://github.com\nA user account: https://github.com/brunj7\nAn organization account: https://github.com/nceas\nNCEAS GitHub instance: https://github.nceas.ucsb.edu/\n\n\n\nThis screen shows the copy of a repository stored on GitHub, with its list of files, when the files and directories were last modified, and some information on who made the most recent changes.\n\nIf we drill into the “commits” for the repository, we can see the history of changes made to all of the files. Looks like kellijohnson and seananderson were fixing things in June and July:\n\nAnd finally, if we drill into the changes made on June 13, we can see exactly what was changed in each file:\n Tracking these changes, and seeing how they relate to released versions of software and files is exactly what Git and GitHub are good for. We will show how they can really be effective for tracking versions of scientific code, figures, and manuscripts to accomplish a reproducible workflow."
  },
  {
    "objectID": "day1-hands-on_drawings_p2.html",
    "href": "day1-hands-on_drawings_p2.html",
    "title": "Hands-on: Planning your work",
    "section": "",
    "text": "Actually our team also collected data for an adjacent watershed!!\n\n\n\n\n\n\n\n\n\n=&gt; How should you modify your workflow to handle this new situation? (10min)"
  },
  {
    "objectID": "day1-hands-on_drawings_p2.html#good-news-we-found-data-for-one-more-watershed",
    "href": "day1-hands-on_drawings_p2.html#good-news-we-found-data-for-one-more-watershed",
    "title": "Hands-on: Planning your work",
    "section": "",
    "text": "Actually our team also collected data for an adjacent watershed!!\n\n\n\n\n\n\n\n\n\n=&gt; How should you modify your workflow to handle this new situation? (10min)"
  },
  {
    "objectID": "day1-hands-on_drawings_p2.html#this-is-your-lucky-day",
    "href": "day1-hands-on_drawings_p2.html#this-is-your-lucky-day",
    "title": "Hands-on: Planning your work",
    "section": "This is your lucky day!!",
    "text": "This is your lucky day!!\nWe found data for additional chemistry measurements while skimming through the database :)\nWe now want to be able to produce the following plot\n\n\n\n\n\n\n\n\n\n=&gt; How should you modify your workflow to handle this new situation? (5min)"
  },
  {
    "objectID": "day1-github_forking.html",
    "href": "day1-github_forking.html",
    "title": "Collaborative Coding Workflows: Forking",
    "section": "",
    "text": "A fork is a copy of a repository that will be stored under your user account. Forking a repository allows you to freely experiment with changes without affecting the original project. We can create a fork on Github by clicking the “fork” button in the top right corner of our repository web page.\nMost commonly, forks are used to either propose changes to someone else’s project or to use someone else’s project as a starting point for your own idea.\nWhen you are satisfied with your work, you can initiate a Pull Request to initiate discussion about your modifications and requesting to integrate your changes to the main repository. Your commit history allows the original repository administrators to see exactly what changes would be merged if they accept your request. Do this by going to the original repository and clicking the “New pull request” button!\nNext, click “compare across forks”, and use the dropdown menus to select your fork as the “head fork” and the original repository as the “base fork”.\nThen type a title and description for the changes you would like to make. By using GitHub’s @mention syntax in your Pull Request message, you can ask for feedback from specific people or teams.\nThis workflow is recommended when you do not have push/write access to a repository, such as contributing to a open source software or R package, or if you are heavily changing a project."
  },
  {
    "objectID": "day1-github_forking.html#demo-time",
    "href": "day1-github_forking.html#demo-time",
    "title": "Collaborative Coding Workflows: Forking",
    "section": "Demo time",
    "text": "Demo time\nUpdate the greetings R package to add the option to change the color of the background of the text.\nRepo: https://github.com/brunj7/greetings\nWhat I will do:\n\nFork the repository\nAdd functionality to this R package. In this example I will modify the say_aloah() function to allow changing the text background color to blue in addition to the existing green (see here for more about the crayon package)\nOnce done, I will create a Pull Request (PR) to integrate the new changes\nMerge back changes!"
  },
  {
    "objectID": "flex-rmarkdown_syntax.html#the-markdown-syntax",
    "href": "flex-rmarkdown_syntax.html#the-markdown-syntax",
    "title": "Rmarkdown syntax",
    "section": "The Markdown syntax ",
    "text": "The Markdown syntax \nYou will find Markdown to be useful for a lot of various tasks and tools you will use as a data scientist. The good new is that the syntax is very basic and it is easy to get started with. In addition, Markdown files (.md) are text files that can be opened by any text editor and can be easily be versioned. You can also render markdown document using pandoc into various document formats (PDF, html, ….). However it is important to be aware that there are several flavors of Markdown out there that are adding more capabilities to the basic one. For example GitHub has created its own flavor.\nAnother interesting feature of Markdown is that you can use html formatting when you want to do some more advanced formatting. In fact, document styles can be customized with HTML/CSS and math notation can be included using LaTeX or mathjax.\n$\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i$\n\\[\\bar{X}=\\frac{1}{n}\\sum_{i=1}^nX_i\\]\nDuring this flex session, explore what you can do with with Markdown basic\nhttps://www.markdownguide.org/basic-syntax/\nAnd extended:\nhttps://www.markdownguide.org/extended-syntax/\n\nR Markdown\nWhat about R markdown in all of this? Well the R Markdown syntax, or more precisely the knitr package, add extra R oriented features which allows the translate of R code, figures and more into a Markdown document to produce dynamic and reproducible documents."
  },
  {
    "objectID": "day_3.html",
    "href": "day_3.html",
    "title": "Day 3 - Getting data in a programmatic way",
    "section": "",
    "text": "Time (PST)\nActivity\n\n\n\n\n10:00am - 11:00am\nIntroduction to APIs (50min)\n\n\n11:00am - 11:10am\nBreak (10 min)\n\n\n11:10am - 12:30am\nAPIs hands-on (80 min)\n\n\n12:30pm - 1:30pm\nLunch (60 min)\n\n\n1:30pm - 2:00pm\nIntroduction to the group project (30 min)\n\n\n2:00pm - 4:00pm\nGroup Projects working session (120min)"
  },
  {
    "objectID": "day_1.html#further-reading",
    "href": "day_1.html#further-reading",
    "title": "Day 1 - Reproducible & Collaborative Workflows",
    "section": "Further reading",
    "text": "Further reading\nHere are a few selected publications to help you to learn more about these topics.\n\nData and scientific workflow management:\n\nThe Practice of Reproducible Research-Case Studies and Lessons from the Data-Intensive Sciences:\nKitzes, Justin, Daniel Turek, and Fatma Deniz. n.d. http://www.practicereproducibleresearch.org/\nGood enough practices in Scientific Computing:\nhttps://doi.org/10.1371/journal.pcbi.1005510\nScript your analysis:\nhttps://doi.org/10.1038/nj7638-563a\nPrinciples for data analysis workflows:\nhttps://doi.org/10.1371/journal.pcbi.1008770\nBenureau, F.C.Y., Rougier, N.P., 2018. Re-run, Repeat, Reproduce, Reuse, Replicate: Transforming Code into Scientific Contributions. Front. Neuroinform. 0. https://doi.org/10.3389/fninf.2017.00069\nSandve, G.K., Nekrutenko, A., Taylor, J., Hovig, E., 2013. Ten Simple Rules for Reproducible Computational Research. PLOS Computational Biology 9, e1003285. https://doi.org/10.1371/journal.pcbi.1003285\nSome Simple Guidelines for Effective Data Management:\nhttps://doi.org/10.1890/0012-9623-90.2.205\nBasic concepts of data management:\nhttps://www.dataone.org/education-modules\n\n\n\nOpen Science\n\nThe Tao of open science for ecology:\nhttps://doi.org/10.1890/ES14-00402.1\nChallenges and Opportunities of Open Data in Ecology:\nhttps://doi.org/10.1126/science.1197962\n\nScientific computing: Code alert\nhttps://doi.org/10.1038/nj7638-563a\nOur path to better science in less time using open data science tools\nhttps://doi.org/10.1038%2Fs41559-017-0160\nFAIR data guiding principles\nhttps://doi.org/10.1038/sdata.2016.18\nSkills and Knowledge for Data-Intensive Environmental Research https://doi.org/10.1093/biosci/bix025\n\nLet go your data\nhttps://doi.org/10.1038/s41563-019-0539-5\n\n\n\nCollaborative coding\n\nA new grad’s guide to coding as a team - Atlassian: https://www.atlassian.com/blog/wp-content/uploads/HelloWorldEbook.pdf\n10 tips for efficient programming: https://www.devx.com/enterprise/top-10-tips-for-efficient-team-coding.html\nAgile Manifesto: https://moodle2019-20.ua.es/moodle/pluginfile.php/2213/mod_resource/content/2/agile-manifesto.pdf\n\n\n\nCode Review\n\nSmall-Group Code Reviews For Education: https://cacm.acm.org/blogs/blog-cacm/175944-small-group-code-reviews-for-education/fulltext\n\n\n\nBranches\n\nInteractive tutorial to learn more about git branches and more https://learngitbranching.js.org/\n\n\n\nGitHub Workflow\n\nGitHub:\n\nguides on how to use GitHub: https://guides.github.com/\nGitHub from RStudio: http://r-pkgs.had.co.nz/git.html#git-pull\n\nForking:\n\nhttps://help.github.com/articles/fork-a-repo/\nhttps://guides.github.com/activities/forking/\n\nComparison of git repository host services: https://www.git-tower.com/blog/git-hosting-services-compared/\nBranches\n\ninteractive tutorial on branches: http://learngitbranching.js.org/\nusing git in a collaborative environment: https://www.atlassian.com/git/tutorials/comparing-workflows/centralized-workflow https://moodle2019-20.ua.es/moodle/pluginfile.php/2213/mod_resource/content/2/agile-manifesto.pdf\n\n\n\nGit using RStudio\n\nHappy Git and GitHub for the useR:http://happygitwithr.com/\nR packages - Git and GitHub: http://r-pkgs.had.co.nz/git.html#git-init\n\nGit mainly from the command line:\n\nInteractive git 101: https://try.github.io/\nVery good tutorial about git: https://www.atlassian.com/git/tutorials/what-is-version-control\nGit tutorial geared towards scientists: http://nyuccl.org/pages/gittutorial/\nShort intro to git basics: https://github.com/mbjones/gitbasics\nGit documentation about the basics: http://gitref.org/basic/\nGit documentation - the basics: https://git-scm.com/book/en/v2/Getting-Started-Git-Basics\nGit terminology: https://www.atlassian.com/git/glossary/terminology\nIn trouble, guide to know what to do: http://justinhileman.info/article/git-pretty/git-pretty.png\nWant to undo something? https://github.com/blog/2019-how-to-undo-almost-anything-with-git\nGit terminology: https://www.atlassian.com/git/glossary/terminology\n8 tips to work better with git: https://about.gitlab.com/2015/02/19/8-tips-to-help-you-work-better-with-git/\nGitPro book (2nd edition): https://git-scm.com/book/en/v2\n\n\n\nUndoing things\n\nHelp to decide how to undo your problem: http://justinhileman.info/article/git-pretty/git-pretty.png\nUndo almost everything with git https://blog.github.com/2015-06-08-how-to-undo-almost-anything-with-git/\nDifference between git reset soft, mixed and hard https://davidzych.com/difference-between-git-reset-soft-mixed-and-hard/\nResetting, Checking Out & Reverting https://www.atlassian.com/git/tutorials/resetting-checking-out-and-reverting"
  },
  {
    "objectID": "day4-pair_programming.html",
    "href": "day4-pair_programming.html",
    "title": "Pair Programming",
    "section": "",
    "text": "Pair programming is an synchronous team activity, where several programmers get to work together on the same piece of code. This is a great way to gain a better sense of what coding style people are using and better understand their way of solving challenges. It is also a great way to learn from each other. Generally, there is one Driver who is the person typing at the computer. The other role is called Navigator(s). The Navigator does not write code and focuses on finding solutions to the problem. Their use of computer should be limited to searching online for solutions."
  },
  {
    "objectID": "day4-pair_programming.html#basic-principles-practices",
    "href": "day4-pair_programming.html#basic-principles-practices",
    "title": "Pair Programming",
    "section": "Basic principles & practices",
    "text": "Basic principles & practices\nAdapted from Woody Zuill https://www.agileconnection.com/article/getting-started-mob-programming\n\nTreat each other with kindness, consideration, and respect - makes group work more fun and sustainable\nDriver/navigator pair programming adapted to work with the whole team - “For an idea to go from your head into the computer, it must go through someone else’s hands.” Speak at the highest level of abstraction that the driver (and the rest of the team) is able to digest at the moment\nTimed Rotation - 20-60 minutes. We don’t require that everyone take the driver role; it is everyone’s choice whether to do so\nWhole Team - every contributor to the project is an integral part of the whole team; when we don’t have the skills we need within the team, we find someone who does and invite them to work with us to accomplish the needed work\nReflect, Tune, and Adjust Frequently - based on agile principle: “At regular intervals, the team reflects on how to become more effective, then tunes and adjusts its behavior accordingly.”"
  },
  {
    "objectID": "day4-pair_programming.html#tips-and-tricks-for-effective-team-programming",
    "href": "day4-pair_programming.html#tips-and-tricks-for-effective-team-programming",
    "title": "Pair Programming",
    "section": "Tips and Tricks for Effective Team Programming",
    "text": "Tips and Tricks for Effective Team Programming\nAdapted from Corey Johannsen: https://blog.newrelic.com/2017/10/31/mob-programming-hurdles/\n\nSuggest, don’t dictate: Instead of telling the driver what to type into their editor, we explain what we’re trying to accomplish and then help the driver find the best solution. We’ve found that drivers learn better this way, and they don’t just end up feeling like a stenographer. Whenever possible, we ask questions that lead the driver to discover the answers on their own.\nStay focused and be present: Shut your laptop and put your phone away. I’ve struggled with following this guideline—we all have—and I recognize that the distraction almost always affects the rest of the mob. We tell all our mob members to be present, and if you can’t, it’s OK to leave until you can be.\nUse a timer, but be ready to pause it: We switch drivers every 20 - 60 minutes. However, we often wander off implementation into design discussions—it’s unavoidable—so this is when we pause the timer. This is another key guideline of our mob: the time you spend driving should be dedicated to writing the code that helps complete the task, not discussing design solutions.\nSet specific tasks for each session: When our mob gathers for a session, we first agree on and create a checklist of the tasks we are going to complete, and order them by priority on a whiteboard. This ensures we are all focused on the same task and keeps us moving forward. Additionally, this keeps us aligned with Minimal Marketable Feature (MMF) work, which we can communicate with our engineering and product managers to assure them we’re completing tasks that align with developing small, self-contained features that demonstrate immediate customer value."
  },
  {
    "objectID": "day4-pair_programming.html#aknowledgements",
    "href": "day4-pair_programming.html#aknowledgements",
    "title": "Pair Programming",
    "section": "Aknowledgements",
    "text": "Aknowledgements\nThis section reuses a lot of materials from an R Meetup organized by the Santa Barbara R Users group (https://github.com/R-Meetup-SB/hackathon-201806), including material prepared by Irene Steves."
  },
  {
    "objectID": "day2-server_collaboration.html",
    "href": "day2-server_collaboration.html",
    "title": "Collaborating on a server",
    "section": "",
    "text": "Collaborative Setup\nThere are additional reasons of particular importance in a collaborative set up, such as a working group:\n\nCentralizing data management: As you know environmental data science is data intensive and often require to deal with a large number of heterogeneous data files. It can be complicated to make sure every collaborators as access to all the data they need. It is even harder to ensure that the exact same version of the data is used by everyone. Moving your workflow to a server, will allow to centralize your data management by having only one copy of the data that you can share with all your collaborators. It will ensure everybody is using the same version of the data and since everyone can access the same data from the same shred folder, everybody will have the exact same path in their script!!\nMake sure your files are safe: Generally, servers are managed by a System Administrator. This person is in charge of keeping the server up-to-date, secured from malwares and set up back up strategies to ensure all the files on the server are backed up. When using cloud solutions, you should always check if a back up plan is available for the resources your using.\n\n\n\nWhat does working on a remote server mean?\nWhat does it mean for your workflow? The good news is that RStudio Server makes it very easy for RStudio users to start using a server for their analysis. The main changes are about:\n\nFile management: you will need to learn to move files (including your R scripts) to the server\nR Package installation: You can still install the R packages you need under your user (with some limitations). However some R packages will be already installed at the server level.\nCode: you will use version control (such as git) and code repository (such as GitHub)to move you code from one machine to another. Do not store code in shared folder, but rather under your home directory and use the code repository solution to share your code with your collaborator and leverage version control to manage the various contributions to your scripts.\n\n\n\n\n\n\nCollaborative workflow on an analytical server; scripts are managed via git and GitHub with each collaborator having a copy of the repository under their home folder, data sets management is centralized in shared folders on ther server"
  },
  {
    "objectID": "day2-cli_advanced.html",
    "href": "day2-cli_advanced.html",
    "title": "Working on a remote server",
    "section": "",
    "text": "We will not cover this during class, it is for your reference. You will also have the opportunity to further practice and learn about the command line in EDS-215.\n\n\nFrom the gitbash (MS Windows) or the terminal (Mac) type:\nssh taylor.bren.ucsb.edu\nYou will be prompted for your username and password.\n\n\n\naurora_ssh\n\n\nYou can also directly add your username:\nssh brun@taylor.bren.ucsb.edu\nIn this case, you will be only asked for your password as you already specified which user you want to connect with.\n\n\n\n\nWho else is logged into this machine? who\nWho is logged into “this shell”? whoami\n\n\n\n\n\n&lt;command&gt; -h, &lt;command&gt; --help\nman, info, apropos, whereis\nSearch the web!\n\n\n\n\nShow me my Rmarkdown files!\nfind . -iname '*.Rmd'\nWhich files are larger than 1GB?\nfind . -size +1G\nWith more details about the files:\nfind . -size +1G -ls\n\n\n\n\n\n\nCancel (abort) a command: Ctrl-c Note: very different than Windows!!\nStop (suspend) a command: Ctrl-z\nCtrl-z can be used to suspend, then background a process\n\n\n\n\n\nLike Windows Task Manager, OSX Activity Monitor\ntop, ps, jobs (hit q to get out!)\nkill to delete an unwanted job or process\nForeground and background: &\n\n\n\n\n\nHow much storage is available on this system? df -h\nHow much storage am “I” using overall? du -hs &lt;folder&gt;\nHow much storage am “I” using, by sub directory? du -h &lt;folder&gt;\n\n\n\n\n\nSee your command history: history\nRe-run last command: !! (pronounced “bang-bang”)\nRe-run 32th command: !32\nRe-run 5th from last command: !-5\nRe-run last command that started with ‘c’: !c\n\n\n\n\n\n\nwc count lines, words, and/or characters\ndiff compare two files for differences\nsort sort lines in a file\nuniq report or filter out repeated lines in a file\n\n\n\n\n\n\n\nstdin, stdout, stderr\n\n\n$ ls *.png | wc -l\n$ ls *.png | wc -l &gt; pngcount.txt\n$ diff &lt;(sort file1.txt) &lt;(sort file2.txt)\n$ ls foo 2&gt;/dev/null\n\nnote use of * as character wildcard for zero or more matches (same in Mac and Windows)\n? matches single character; _ is SQL query equivalent\n\n\n\n\n\n\n\nvim\nemacs\nnano\n\n$ nano .bashrc\n\n\n\n\n\ngrep search files for text\nsed filter and transform text\nfind advanced search for files/directories\n\n\n\nShow all lines containing “bug” in my R scripts\n$ grep bug *.R\nNow count the number of occurrences per file\n$ grep -c bug *.R\nPrint the names of files that contain bug\n$ grep -l bug *.R\n\n\n\n\ncat print file(s)\nhead print first few lines of file(s)\ntail print last few lines of file(s)\nless “pager” – view file interactively (type q to quit command)\nod --t “octal dump” – to view file’s underlying binary/octal/hexadecimal/ASCII format\n\n$ brun@aurora:~/data$ head -3 env.csv\nEnvID,LocID,MinDate,MaxDate,AnnPPT,MAT,MaxAT,MinAT,WeatherS,Comments\n1,*Loc ID,-888,-888,-888,-888,-888,-888,-888,-888\n1,10101,-888,-888,-888,-888,-888,-888,-888,-888\n\n$ brun@aurora:~/data$ head -3 env.csv | od -cx\n0000000   E   n   v   I   D   ,   L   o   c   I   D   ,   M   i   n   D\n           6e45    4976    2c44    6f4c    4963    2c44    694d    446e\n0000020   a   t   e   ,   M   a   x   D   a   t   e   ,   A   n   n   P\n           7461    2c65    614d    4478    7461    2c65    6e41    506e\n0000040   P   T   ,   M   A   T   ,   M   a   x   A   T   ,   M   i   n\n           5450    4d2c    5441    4d2c    7861    5441    4d2c    6e69\n0000060   A   T   ,   W   e   a   t   h   e   r   S   ,   C   o   m   m\n           5441    572c    6165    6874    7265    2c53    6f43    6d6d\n0000100   e   n   t   s  \\r  \\n   1   ,   *   L   o   c       I   D   ,\n           6e65    7374    0a0d    2c31    4c2a    636f    4920    2c44\n0000120   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -\n           382d    3838    2d2c    3838    2c38    382d    3838    2d2c\n0000140   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -   8\n           3838    2c38    382d    3838    2d2c    3838    2c38    382d\n0000160   8   8   ,   -   8   8   8  \\r  \\n   1   ,   1   0   1   0   1\n           3838    2d2c    3838    0d38    310a    312c    3130    3130\n0000200   ,   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,\n           2d2c    3838    2c38    382d    3838    2d2c    3838    2c38\n0000220   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -\n           382d    3838    2d2c    3838    2c38    382d    3838    2d2c\n0000240   8   8   8   ,   -   8   8   8  \\r  \\n\n           3838    2c38    382d    3838    0a0d\n\n\n\n\nalias lwc='ls *.jpg | wc -l'\nYou can create a number of custom aliases that are available whenever you log in, by putting commands such as the above in your shell start-up file, e.g. .bashrc"
  },
  {
    "objectID": "day2-cli_advanced.html#connecting-to-a-remote-server-via-ssh",
    "href": "day2-cli_advanced.html#connecting-to-a-remote-server-via-ssh",
    "title": "Working on a remote server",
    "section": "",
    "text": "From the gitbash (MS Windows) or the terminal (Mac) type:\nssh taylor.bren.ucsb.edu\nYou will be prompted for your username and password.\n\n\n\naurora_ssh\n\n\nYou can also directly add your username:\nssh brun@taylor.bren.ucsb.edu\nIn this case, you will be only asked for your password as you already specified which user you want to connect with."
  },
  {
    "objectID": "day2-cli_advanced.html#unix-systems-are-multi-user",
    "href": "day2-cli_advanced.html#unix-systems-are-multi-user",
    "title": "Working on a remote server",
    "section": "",
    "text": "Who else is logged into this machine? who\nWho is logged into “this shell”? whoami"
  },
  {
    "objectID": "day2-cli_advanced.html#getting-help",
    "href": "day2-cli_advanced.html#getting-help",
    "title": "Working on a remote server",
    "section": "",
    "text": "&lt;command&gt; -h, &lt;command&gt; --help\nman, info, apropos, whereis\nSearch the web!"
  },
  {
    "objectID": "day2-cli_advanced.html#finding-stuff",
    "href": "day2-cli_advanced.html#finding-stuff",
    "title": "Working on a remote server",
    "section": "",
    "text": "Show me my Rmarkdown files!\nfind . -iname '*.Rmd'\nWhich files are larger than 1GB?\nfind . -size +1G\nWith more details about the files:\nfind . -size +1G -ls"
  },
  {
    "objectID": "day2-cli_advanced.html#getting-things-done",
    "href": "day2-cli_advanced.html#getting-things-done",
    "title": "Working on a remote server",
    "section": "",
    "text": "Cancel (abort) a command: Ctrl-c Note: very different than Windows!!\nStop (suspend) a command: Ctrl-z\nCtrl-z can be used to suspend, then background a process\n\n\n\n\n\nLike Windows Task Manager, OSX Activity Monitor\ntop, ps, jobs (hit q to get out!)\nkill to delete an unwanted job or process\nForeground and background: &\n\n\n\n\n\nHow much storage is available on this system? df -h\nHow much storage am “I” using overall? du -hs &lt;folder&gt;\nHow much storage am “I” using, by sub directory? du -h &lt;folder&gt;\n\n\n\n\n\nSee your command history: history\nRe-run last command: !! (pronounced “bang-bang”)\nRe-run 32th command: !32\nRe-run 5th from last command: !-5\nRe-run last command that started with ‘c’: !c"
  },
  {
    "objectID": "day2-cli_advanced.html#a-sampling-of-simple-commands-for-dealing-with-files",
    "href": "day2-cli_advanced.html#a-sampling-of-simple-commands-for-dealing-with-files",
    "title": "Working on a remote server",
    "section": "",
    "text": "wc count lines, words, and/or characters\ndiff compare two files for differences\nsort sort lines in a file\nuniq report or filter out repeated lines in a file"
  },
  {
    "objectID": "day2-cli_advanced.html#get-into-the-flow-with-pipes",
    "href": "day2-cli_advanced.html#get-into-the-flow-with-pipes",
    "title": "Working on a remote server",
    "section": "",
    "text": "stdin, stdout, stderr\n\n\n$ ls *.png | wc -l\n$ ls *.png | wc -l &gt; pngcount.txt\n$ diff &lt;(sort file1.txt) &lt;(sort file2.txt)\n$ ls foo 2&gt;/dev/null\n\nnote use of * as character wildcard for zero or more matches (same in Mac and Windows)\n? matches single character; _ is SQL query equivalent"
  },
  {
    "objectID": "day2-cli_advanced.html#text-editing",
    "href": "day2-cli_advanced.html#text-editing",
    "title": "Working on a remote server",
    "section": "",
    "text": "vim\nemacs\nnano\n\n$ nano .bashrc"
  },
  {
    "objectID": "day2-cli_advanced.html#searching-advanced-utilities",
    "href": "day2-cli_advanced.html#searching-advanced-utilities",
    "title": "Working on a remote server",
    "section": "",
    "text": "grep search files for text\nsed filter and transform text\nfind advanced search for files/directories\n\n\n\nShow all lines containing “bug” in my R scripts\n$ grep bug *.R\nNow count the number of occurrences per file\n$ grep -c bug *.R\nPrint the names of files that contain bug\n$ grep -l bug *.R\n\n\n\n\ncat print file(s)\nhead print first few lines of file(s)\ntail print last few lines of file(s)\nless “pager” – view file interactively (type q to quit command)\nod --t “octal dump” – to view file’s underlying binary/octal/hexadecimal/ASCII format\n\n$ brun@aurora:~/data$ head -3 env.csv\nEnvID,LocID,MinDate,MaxDate,AnnPPT,MAT,MaxAT,MinAT,WeatherS,Comments\n1,*Loc ID,-888,-888,-888,-888,-888,-888,-888,-888\n1,10101,-888,-888,-888,-888,-888,-888,-888,-888\n\n$ brun@aurora:~/data$ head -3 env.csv | od -cx\n0000000   E   n   v   I   D   ,   L   o   c   I   D   ,   M   i   n   D\n           6e45    4976    2c44    6f4c    4963    2c44    694d    446e\n0000020   a   t   e   ,   M   a   x   D   a   t   e   ,   A   n   n   P\n           7461    2c65    614d    4478    7461    2c65    6e41    506e\n0000040   P   T   ,   M   A   T   ,   M   a   x   A   T   ,   M   i   n\n           5450    4d2c    5441    4d2c    7861    5441    4d2c    6e69\n0000060   A   T   ,   W   e   a   t   h   e   r   S   ,   C   o   m   m\n           5441    572c    6165    6874    7265    2c53    6f43    6d6d\n0000100   e   n   t   s  \\r  \\n   1   ,   *   L   o   c       I   D   ,\n           6e65    7374    0a0d    2c31    4c2a    636f    4920    2c44\n0000120   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -\n           382d    3838    2d2c    3838    2c38    382d    3838    2d2c\n0000140   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -   8\n           3838    2c38    382d    3838    2d2c    3838    2c38    382d\n0000160   8   8   ,   -   8   8   8  \\r  \\n   1   ,   1   0   1   0   1\n           3838    2d2c    3838    0d38    310a    312c    3130    3130\n0000200   ,   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,\n           2d2c    3838    2c38    382d    3838    2d2c    3838    2c38\n0000220   -   8   8   8   ,   -   8   8   8   ,   -   8   8   8   ,   -\n           382d    3838    2d2c    3838    2c38    382d    3838    2d2c\n0000240   8   8   8   ,   -   8   8   8  \\r  \\n\n           3838    2c38    382d    3838    0a0d"
  },
  {
    "objectID": "day2-cli_advanced.html#create-custom-commands-with-alias",
    "href": "day2-cli_advanced.html#create-custom-commands-with-alias",
    "title": "Working on a remote server",
    "section": "",
    "text": "alias lwc='ls *.jpg | wc -l'\nYou can create a number of custom aliases that are available whenever you log in, by putting commands such as the above in your shell start-up file, e.g. .bashrc"
  },
  {
    "objectID": "day1-coding_together.html",
    "href": "day1-coding_together.html",
    "title": "Coding together",
    "section": "",
    "text": "In this part of the lesson, you will learn:\n\nWhy git is useful for reproducible analysis\nHow to use git to track changes to your work over time\nHow to use GitHub to collaborate with others\nHow to write effective commit messages\nHow to structure your commits so your changes are clear to others\nHow to fork a repository to contribute to its content\nHow to create a pull request\nHow to review a pull request"
  },
  {
    "objectID": "day1-coding_together.html#learning-objectives",
    "href": "day1-coding_together.html#learning-objectives",
    "title": "Coding together",
    "section": "",
    "text": "In this part of the lesson, you will learn:\n\nWhy git is useful for reproducible analysis\nHow to use git to track changes to your work over time\nHow to use GitHub to collaborate with others\nHow to write effective commit messages\nHow to structure your commits so your changes are clear to others\nHow to fork a repository to contribute to its content\nHow to create a pull request\nHow to review a pull request"
  },
  {
    "objectID": "day1-coding_together.html#why-collaborative-coding",
    "href": "day1-coding_together.html#why-collaborative-coding",
    "title": "Coding together",
    "section": "Why collaborative coding",
    "text": "Why collaborative coding\n\nSlide deck\nEnvironmental Data Science (EDS), as many other data-driven research fields, requires a transdisciplinary approach to tackle challenges that often span across several domains of expertise. Working as a team will leverage know-how from diverse collaborators and be the most efficient way to tackle complex problems in EDS. Consequently collaborative skills are required to work effectively as a member of a team. No matter their focus, highly effective teams share certain characteristics:\n\nRight size\nDiverse group of people with the right mix of skills, knowledge, and competencies\nAligned purpose and incentives\nEffective organizational structure\nStrong individual contributions\nSupportive team processes and culture\n\nSince Analytical Workflows are rarely linear! and are developed iteratively, the most efficient way to iterate quickly on your analysis is to use scripts and leave copy-pasting behind. Programming as part of a team is different than writing a script for your(present)self. However learning programming as part of a team is not only critical to the efficacy of your team, it will also you help you to grow as a programmer by:\n\nMotivating you to document well your work\nHelping you to think how to make your work reusable (by you, your future you and others)\nLearning to read code from collaborators to build upon each others work\nGain further knowledge in software development tools, such as version control\n\nDeveloping those skills will accelerate your research and open the door for you to contribute to open source projects."
  },
  {
    "objectID": "day1-coding_together.html#how-to-code-together",
    "href": "day1-coding_together.html#how-to-code-together",
    "title": "Coding together",
    "section": "How to code together",
    "text": "How to code together\nIt is important to acknowledge that there are many solutions to the complex research questions you will be facing in EDS. Each of those solutions will have several possible implementations, meaning that more likely you might code this implementation differently than your collaborators. Integrated software engineer teams generally try to mitigate this by developing coding standards and conventions that will guide how to write code and develop specific implementation. In scientific teams in which the collaboration is more loose and maybe more ephemeral as well, developing detailed coding standards will be too much of an overhead. However, we think it is important to acknowledge that coding style may varies among the data scientists of a project and it is a good discussion to have among the team at the beginning of the project. For example, in R it could be trying to use the tidyverse approach as much as possible. We also think there are two activities that will make the team more efficient: Code Review and Pair Programming.\n\nTools\nThe good news is there are several tools out there that have been designed to make developing code as a team more efficient. In this course, we will focus on getting familiar with the following:\n\nVersion control system: say goodbye to save as\nCode repository: where we share code and communicate ideas and feedback"
  },
  {
    "objectID": "resources.html",
    "href": "resources.html",
    "title": "Course resources",
    "section": "",
    "text": "More References\n\nThe Practice of Reproducible Research-Case Studies and Lessons from the Data-Intensive Sciences:\nKitzes, Justin, Daniel Turek, and Fatma Deniz. n.d. http://www.practicereproducibleresearch.org/\nReproducible Research with R and RStudio, Chrisotpher Gandrud http://christophergandrud.github.io/RepResR-RStudio/index.html\nInitial steps toward reproducible research, Karl Broman, https://kbroman.org/steps2rr/\nReproducibility guide from ROpenSci https://ropensci.github.io/reproducibility-guide/\nThe Turing Way Community. (2022). The Turing Way: A handbook for reproducible, ethical and collaborative research. Zenodo. doi: 10.5281/zenodo.3233853"
  },
  {
    "objectID": "day3-api.html",
    "href": "day3-api.html",
    "title": "Getting Data using APIs",
    "section": "",
    "text": "Using website to retrieve data is great interactive way to search and explore data of interest. Let’s start our day by searching some data on the data repository DateONE: https://search.dataone.org/data\nIn case you need some inspiration, look at this dataset about historic precipitation in Alaska: https://doi.org/10.5063/N29VCQ"
  },
  {
    "objectID": "day3-api.html#the-manual-way",
    "href": "day3-api.html#the-manual-way",
    "title": "Getting Data using APIs",
    "section": "",
    "text": "Using website to retrieve data is great interactive way to search and explore data of interest. Let’s start our day by searching some data on the data repository DateONE: https://search.dataone.org/data\nIn case you need some inspiration, look at this dataset about historic precipitation in Alaska: https://doi.org/10.5063/N29VCQ"
  },
  {
    "objectID": "day3-api.html#the-programmatic-way",
    "href": "day3-api.html#the-programmatic-way",
    "title": "Getting Data using APIs",
    "section": "The programmatic way",
    "text": "The programmatic way\nAlthough discovering data through a web interface is convenient and offers a great experience, it is often hard to scale this approach or to integrate it into a reproducible workflow.\nSlides"
  },
  {
    "objectID": "day2-server_uploading.html",
    "href": "day2-server_uploading.html",
    "title": "Uploading things to a server",
    "section": "",
    "text": "You have several options to upload files to the server. Some are more convenient if you have few files, like RStudio interface, some are more built for uploading a lot of files at one, like specific software… and you guessed it the CLI :)"
  },
  {
    "objectID": "day2-server_uploading.html#sftp-software",
    "href": "day2-server_uploading.html#sftp-software",
    "title": "Uploading things to a server",
    "section": "sFTP Software",
    "text": "sFTP Software\nAn efficient protocol to upload files is FTP (File Transfer Protocol). The s stands for secured. Any software supporting those protocols will work to transfer files.\nWe recommend the following free software:\n\nMac users: cyberduck\nWindows: WinSCP"
  },
  {
    "objectID": "day2-server_uploading.html#scp",
    "href": "day2-server_uploading.html#scp",
    "title": "Uploading things to a server",
    "section": "scp",
    "text": "scp\nThe scp command is another convenient way to transfer a single file or directory using the CLI. You can run it from Taylor or from your local computer. Here is the basic syntax:\nscp /source/path hostname:/path/to/destination/\nHere is an example of my uploading the file 10min-loop.R to Taylor from my laptop. The destination directory on Taylor is /Users/brun/:\nscp 10min-loop.R brun@taylor.bren.ucsb.edu:/Users/brun/\nBTW try to open and run that script for fun!!\nIf you want to upload an entire folder, you can add the -r option to the command. The general syntax is:\nscp -r /path/to/source-folder user@server:/path/to/destination-folder/\nHere is an example uploading all the images in the myplot folder\nscp -r myplot brun@taylor.bren.ucsb.edu:/Users/brun/plots"
  },
  {
    "objectID": "day1-github_collab.html",
    "href": "day1-github_collab.html",
    "title": "GitHub Collaboration",
    "section": "",
    "text": "Collaborating through forking\nMaterials\n\n\nCollaborating through branches\nMaterials\n\n\nInteractive session 2: Collaborative coding with GitHub\nhttps://github.com/brunj7/eds214-handson-ghcollab"
  },
  {
    "objectID": "code_of_conduct.html",
    "href": "code_of_conduct.html",
    "title": "Code of Conduct",
    "section": "",
    "text": "All enrolled students, auditors, and course visitors are expected to comply with the following code of conduct. We expect cooperation from all members to help ensure a safe and welcoming environment for everybody.\n\nOverview\nWe are determined to make our courses welcoming, inclusive and harassment-free for everyone regardless of gender, gender identity and expression, race, age, sexual orientation, disability, physical appearance, body size, or religion (or lack thereof). We do not tolerate harassment of class participants, teaching assistants, or instructors in any form. Derogatory, abusive, demeaning or sexual language and imagery is not appropriate or acceptable. Saying something “as a joke” does not make it less offensive, harmful, or consequential.\nAnything not covered here but that exists in the UCSB Student Conduct Code also applies, and will be enforced by UCSB Policy.\nThese expectations and consequences apply to synchronous discussions, office hours, the course Slack workspace, and all other modes of communication, posting or discussion by course participants.\n\n\nExamples of behavior that contributes to creating a positive environment include:\n\nUsing welcoming and inclusive language\nBeing an aware and respectful colleague (raise your hand when asked, respect others’ time and space, include peers in small discussions, don’t dominate meetings, etc.)\nGiving proper credit to the creator (of an idea/material/solution/etc.)\nBeing respectful of differing viewpoints and experiences\nShowing empathy towards all community members\nUnderstanding that an individual’s experience and worldview are influenced by multiple (and often compounding) facets of their identity, and that your perception of a situation/topic/reaction may be very different from your classmates’\n\n\n\nExamples of unacceptable behavior by class participants include:\n\nDistracting other students in classes in labs, or otherwise distracting from their education\nAny abuse, disrespect or harassment of teaching assistants, other students, or teachers, is not tolerated and will result in disciplinary action as needed\nThe use of unwelcome sexual attention or advances\nTrolling, insulting/derogatory comments or language, and personal or political attacks\nPublic or private harassment\nPublishing others’ private information, such as a physical or electronic address, without explicit permission\nOther conduct which could reasonably be considered inappropriate in a professional setting\n\nMembers asked to stop any harassing behavior are expected to comply immediately. If you are being harassed, notice that someone else is being harassed, or have any other concerns in or related to these classes, you are welcome to contact Allison or use outside resources.\n\n\nOther resources at UCSB\nAt Campus Advocacy, Resources and Education (CARE) you can chat with a UCSB staff member in a confidential setting. CARE assists faculty, staff and students who have been impacted by sexual harassment, sexual assault, domestic/dating violence, and stalking. The office is confidential so you can talk with a staff member in private without any reporting obligations.\n\nPhone: (805) 893-4613.\nOffice is located in the Student Resource Building (SRB) near parking lot 23.\n\nAnother confidential resource is the Campus Ombuds office. The Ombuds office is particularly helpful if you would like to describe a sensitive issue in a confidential setting and learn more about campus resources to address the issue. The Ombuds office is located at 1205-K Girvetz Hall and their phone number is 805-893-3285. The Ombuds office provides consultation, mediation, and facilitation, among other services, for faculty, staff and students.\nUCSB Academic Counseling is a resource outside of the Bren department that can help with a number of topics from academic planning to “balancing personal difficulties in academics.”\n\nGraduate counselor: Ryan Sims\nPhone: 805-893-2068\nEmail: ryan.sims@graddiv.ucsb.edu\n\nThis is a living document, that we are always hoping to improve. If you have suggestions, questions or ideas for how we can update our Code of Conduct, we encourage you to reach out to us and will be grateful for your feedback.\nContributions by:\n\nAllison Horst\nJessica Couture {“mode”:“full”,“isActive”:false}"
  },
  {
    "objectID": "day1-git-collaboration-conflicts.html",
    "href": "day1-git-collaboration-conflicts.html",
    "title": "git conflicts",
    "section": "",
    "text": "In this lesson, you will learn:\n\nWhat typically causes conflicts when collaborating\nWorkflows to avoid conflicts\nHow to resolve a conflict"
  },
  {
    "objectID": "day1-git-collaboration-conflicts.html#learning-objectives",
    "href": "day1-git-collaboration-conflicts.html#learning-objectives",
    "title": "git conflicts",
    "section": "",
    "text": "In this lesson, you will learn:\n\nWhat typically causes conflicts when collaborating\nWorkflows to avoid conflicts\nHow to resolve a conflict"
  },
  {
    "objectID": "day1-git-collaboration-conflicts.html#before-we-start",
    "href": "day1-git-collaboration-conflicts.html#before-we-start",
    "title": "git conflicts",
    "section": "Before we start",
    "text": "Before we start\nFirst thing to know is that actually git pull is a two step process: git pull = git fetch + git merge\nSecond: you did nothing wrong!! Git tries to merge files automatically. When the changes are on the same file far apart, git will figure it out on his own and do the merge automatically. However if changes are overlapping, git will call you to the rescue and ask you how to best merge the two versions."
  },
  {
    "objectID": "day1-git-collaboration-conflicts.html#merge-conflicts",
    "href": "day1-git-collaboration-conflicts.html#merge-conflicts",
    "title": "git conflicts",
    "section": "Merge conflicts",
    "text": "Merge conflicts\nSo things can go wrong, which usually starts with a merge conflict, due to both collaborators making incompatible changes to a file. While the error messages from merge conflicts can be daunting, getting things back to a normal state can be straightforward once you’ve got an idea where the problem lies.\nThis is most easily avoided by good communication about who is working on various sections of each file, and trying to avoid overlaps. But sometimes it happens, and git is there to warn you about potential problems. And git will not allow you to overwrite one person’s changes to a file with another’s changes to the same file if they were based on the same version.\n\nThe main problem with merge conflicts is that, when the Owner and Collaborator both make changes to the same line of a file, git doesn’t know whose changes take precedence. You have to tell git whose changes to use for that line."
  },
  {
    "objectID": "day1-git-collaboration-conflicts.html#how-to-resolve-a-conflict",
    "href": "day1-git-collaboration-conflicts.html#how-to-resolve-a-conflict",
    "title": "git conflicts",
    "section": "How to resolve a conflict",
    "text": "How to resolve a conflict\n\nAbort, abort, abort…\nSometimes you just made a mistake. When you get a merge conflict, the repository is placed in a ‘Merging’ state until you resolve it. There’s a command line command to abort doing the merge altogether:\ngit merge --abort\nOf course, after doing that you still haven’t synced with your collaborator’s changes, so things are still unresolved. But at least your repository is now usable on your local machine.\n\n\nCheckout\nThe simplest way to resolve a conflict, given that you know whose version of the file you want to keep, is to use the command line git program to tell git to use either your changes (the person doing the merge), or their changes (the other collaborator).\n\nkeep your collaborators file: git checkout --theirs conflicted_file.Rmd\nkeep your own file: git checkout --ours conflicted_file.Rmd\n\nOnce you have run that command, then run add, commit, and push the changes as normal.\n\n\nPull and edit the file\nBut that requires the command line. If you want to resolve from RStudio, or if you want to pick and choose some of your changes and some of your collaborator’s, then instead you can manually edit and fix the file. When you pulled the file with a conflict, git notices that there is a conflict and modifies the file to show both your own changes and your collaborator’s changes in the file. It also shows the file in the Git tab with an orange U icon, which indicates that the file is Unmerged, and therefore awaiting you help to resolve the conflict. It delimits these blocks with a series of less than and greater than signs, so they are easy to find:\n\nTo resolve the conficts, simply find all of these blocks, and edit them so that the file looks how you want (either pick your lines, your collaborators lines, some combination, or something altogether new), and save. Be sure you removed the delimiter lines that started with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;.\nOnce you have made those changes, you simply add, commit, and push the files to resolve the conflict.\n\n\nProducing and resolving merge conflicts\nTo illustrate this process, we’re going to carefully create a merge conflict step by step, show how to resolve it, and show how to see the results of the successful merge after it is complete. First, we will walk through the exercise to demonstrate the issues.\n\nOwner and collaborator ensure all changes are updated\nFirst, start the exercise by ensuring that both the Owner and Collaborator have all of the changes synced to their local copies of the Owner’s repositoriy in RStudio. This includes doing a git pull to ensure that you have all changes local, and make sure that the Git tab in RStudio doesn’t show any changes needing to be committed.\n\n\nOwner makes a change and commits\nFrom that clean slate, the Owner first modifies and commits a small change inlcuding their name on a specific line of the README.md file (we will change line 4). Work to only change that one line, and add your username to the line in some form and commit the changes (but DO NOT push). We are now in the situation where the owner has unpushed changes that the collaborator can not yet see.\n\n\nCollaborator makes a change and commits on the same line\nNow the collaborator also makes changes to the same of the README.md file in their RStudio copy of the project, adding their name to the line. They then commit. At this point, both the owner and collaborator have committed changes based on their shared version of the README.md file, but neither has tried to share their changes via GitHub.\n\n\nCollaborator pushes the file to GitHub\nSharing starts when the Collaborator pushes their changes to the GitHub repo, which updates GitHub to their version of the file. The owner is now one revision behind, but doesn’t yet know it.\n\n\nOwner pushes their changes and gets an error\nAt this point, the owner tries to push their change to the repository, which triggers an error from GitHub. While the error message is long, it basically tells you everything needed (that the owner’s repository doesn’t reflect the changes on GitHub, and that they need to pull before they can push).\n\n\n\nOwner pulls from GitHub to get Collaborator changes\nDoing what the message says, the Owner pulls the changes from GitHub, and gets another, different error message. In this case, it indicates that there is a merge conflict because of the conflicting lines.\n\nIn the Git pane of RStudio, the file is also flagged with an orange ‘U’, which stands for an unresolved merge conflict.\n\n\n\nOwner edits the file to resolve the conflict\nTo resolve the conflict, the Owner now needs to edit the file. Again, as indicated above, git has flagged the locations in the file where a conflict occcurred with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;. The Owner should edit the file, merging whatever changes are appropriate until the conflicting lines read how they should, and eliminate all of the marker lines with with &lt;&lt;&lt;&lt;&lt;&lt;&lt;, =======, and &gt;&gt;&gt;&gt;&gt;&gt;&gt;.\n\nOf course, for scripts and programs, resolving the changes means more than just merging the text – whoever is doing the merging should make sure that the code runs properly and none of the logic of the program has been broken.\n\n\n\nOwner commits the resolved changes\nFrom this point forward, things proceed as normal. The owner first ‘Adds’ the file changes to be made, which changes the orange U to a blue M for modified, and then commits the changes locally. The owner now has a resolved version of the file on their system.\n\n\n\nOwner pushes the resolved changes to GitHub\nHave the Owner push the changes, and it should replicate the changes to GitHub without error.\n\n\n\nCollaborator pulls the resolved changes from GitHub\nFinally, the Collaborator can pull from GitHub to get the changes the owner made.\n\n\nBoth can view commit history\nWhen either the Collaborator or the Owner view the history, the conflict, associated branch, and the merged changes are clearly visible in the history.\n\n\n\n\nMerge Conflict Challenge\nNow it’s your turn. In pairs, intentionally create a merge conflict, and then go through the steps needed to resolve the issues and continue developing with the merged files. See the sections above for help with each of these steps:\n\nStep 0: Owner and collaborator ensure all changes are updated\nStep 1: Owner makes a change and commits\nStep 2: Collaborator makes a change and commits on the same line\nStep 3: Collaborator pushes the file to GitHub\nStep 4: Owner pushes their changes and gets an error\nStep 5: Owner pulls from GitHub to get Collaborator changes\nStep 6: Owner edits the file to resolve the conflict\nStep 7: Owner commits the resolved changes\nStep 8: Owner pushes the resolved changes to GitHub\nStep 9: Collaborator pulls the resolved changes from GitHub\nStep 10: Both can view commit history"
  },
  {
    "objectID": "day1-git-collaboration-conflicts.html#workflows-to-avoid-merge-conflicts",
    "href": "day1-git-collaboration-conflicts.html#workflows-to-avoid-merge-conflicts",
    "title": "git conflicts",
    "section": "Workflows to avoid merge conflicts",
    "text": "Workflows to avoid merge conflicts\nSome basic rules of thumb can avoid the vast majority of merge conflicts, saving a lot of time and frustration. These are words our teams live by:\n\nCommunicate often\nTell each other what you are working on\nPull immediately before you commit or push\nCommit often in small chunks.\n\nAlways start your working sessions with a pull to get any outstanding changes, then start doing your editing and work. Stage your changes, but before you commit, Pull again to see if any new changes have arrived. If so, they should merge in easily if you are working in different parts of the program. You can then Commit and immediately Push your changes safely. Good luck, and try to not get frustrated. Once you figure out how to handle merge conflicts, they can be avoided or dispatched when they occur, but it does take a bit of practice."
  },
  {
    "objectID": "day1-git-collaboration-conflicts.html#your-turn",
    "href": "day1-git-collaboration-conflicts.html#your-turn",
    "title": "git conflicts",
    "section": "Your turn",
    "text": "Your turn\n\nUse your favorite desserts to create a merge conflict. Resolve it using RStudio\nMixing the fun: try to create a merge conflict using the forking workflow. How would you do this? Try to resolve the conflict on the GitHub website! hint"
  },
  {
    "objectID": "day2-bash-scripting.html",
    "href": "day2-bash-scripting.html",
    "title": "An introduction to Bash scripting",
    "section": "",
    "text": "So far you’ve seen Bash as a CLI, i.e., as an interactive tool. But Bash is also a programming language, and that is where its real power lies. Nobody claims that Bash is a particularly good general-purpose programming language (though in theory it could be used that way). But when it comes to manipulating files, working in Bash is often far more convenient than working in other languages such as R or Python. An additional benefit is that Bash is almost universally available, whereas R and Python require additional installs.\nIn this exercise you will create a Bash script to perform two common file manipulation tasks: adding header records and renaming files."
  },
  {
    "objectID": "day2-bash-scripting.html#adding-a-header-record-to-each-file",
    "href": "day2-bash-scripting.html#adding-a-header-record-to-each-file",
    "title": "An introduction to Bash scripting",
    "section": "Adding a header record to each file",
    "text": "Adding a header record to each file\nThere are two problems with our state baby name files: they lack header records, and they end in .TXT instead of .csv. To tackle the first problem, we can say:\necho \"state,gender,year,firstname,count\" &gt; tempfile\nThis gives us a header record in a new file tempfile. We can then append the contents of one of our data files to that file:\ncat STATE.AK.TXT &gt;&gt; tempfile\nExamine tempfile using any of the following commands:\nhead tempfile\ncat tempfile\nmore tempfile\nless tempfile\nAt this point we can now rename tempfile to the desired filename ending in .csv and we’re done (with one data file anyway):\nmv tempfile STATE.AK.csv\n\nQuestions\n\n\nWhat will happen if you do this?\necho \"state,gender,year,firstname,count\" &gt; tempfile\ncat STATE.AK.TXT &gt; tempfile\nOr this?\necho \"state,gender,year,firstname,count\" &gt;&gt; tempfile\ncat STATE.AK.TXT &gt; tempfile\nOr this?\necho \"state,gender,year,firstname,count\" &gt;&gt; tempfile\ncat STATE.AK.TXT &gt;&gt; tempfile\n\n\nWe want to do the above processing to every file, that’s where loops come in. But first, we need variables."
  },
  {
    "objectID": "day2-bash-scripting.html#variables",
    "href": "day2-bash-scripting.html#variables",
    "title": "An introduction to Bash scripting",
    "section": "Variables",
    "text": "Variables\nBash supports variables, and variables are essential in writing Bash scripts. To set a variable:\nname=Alice\nNo space between the variable name and equals sign! Variables are referenced as ${name}, or as just $name if not ambiguous.\n\nQuestion\n\nWhat will be printed by the following three echo commands, and why?\nvar=xy\necho $varz\necho ${var}z\necho $var.z\n\nWhen we process our data files in a loop, we will be working with a variable file whose value will be the name of the current file such as STATE.AK.TXT. We will want to construct a new filename such as STATE.AK.csv from that variable. This can be done by modifying a simple variable reference such as ${file} to include Bash string processing functions inside the braces. Here are two ways:\nfile=STATE.AK.TXT\necho ${file/.TXT/.csv}\necho ${file%.TXT}.csv\nThe first form performs a string substitution, substituting the first (and in our case, only) occurrence of .TXT with .csv. The second form peels off the trailing .TXT (% means “trailing”), leaving just STATE.AK. To that we then append .csv.\n\n\n\n\n\n\nTip\n\n\n\nThere are lots of string processing operations, and all are invoked using single characters like %, #, ^, etc. How the heck can you remember that? The answer is, you likely can’t. The takeaway for you in this class is that there exist string processing operators in Bash, and that if you look in the Bash manual you can get a description of what each one does.\n\nBash manual\nBash manual: Shell parameter expansion\nParameter expansion FAQ"
  },
  {
    "objectID": "day2-bash-scripting.html#scripts",
    "href": "day2-bash-scripting.html#scripts",
    "title": "An introduction to Bash scripting",
    "section": "Scripts",
    "text": "Scripts\nA Bash script is a text file containing the same Bash commands you might type interactively. It’s analogous to an R or Python script but written in Bash instead of one of those other languages.\nBash knows it is reading from a file instead of the terminal window, and it operates slightly differently:\n\nIt doesn’t print a prompt.\nIt doesn’t read Bash configuration files (~/.bashrc, ~/.bash_profile, ~/.profile, etc.). As a consequence, aliases and variables defined in those files are not visible to scripts.\n\nA Bash script can be run like so:\nbash myscript.sh\n\nExercise\n\nCreate a script myscript.sh that processes data file STATE.AK.TXT as above.\n\nOnce you’ve created a script, it can be very useful to check it for errors and potential pitfalls by running it through ShellCheck."
  },
  {
    "objectID": "day2-bash-scripting.html#loops",
    "href": "day2-bash-scripting.html#loops",
    "title": "An introduction to Bash scripting",
    "section": "Loops",
    "text": "Loops\nBash supports a few kinds of loops. The one we’ll be using here looks like this:\nfor var in list_of_things; do\n    # operate on $var\ndone\nAlternative syntax (notice the do is on a line by itself):\nfor var in list_of_things\ndo\n    # operate on $var\ndone\nA couple examples:\nfor name in Tom Dick Harry; do\n    echo \"Every $name\"\ndone\nfor i in {99..1}; do\n    echo \"$i bottles of beer on the wall\"\ndone\nIt’s very common to operate on files:\nfor file in *.TXT; do\n    # do something with $file, for example:\n    echo $file\ndone"
  },
  {
    "objectID": "day2-bash-scripting.html#putting-it-all-together",
    "href": "day2-bash-scripting.html#putting-it-all-together",
    "title": "An introduction to Bash scripting",
    "section": "Putting it all together",
    "text": "Putting it all together\nRecall that our goal is to add a header record to each data file, and to rename the data files to .csv. We can do so by writing a loop, and performing those two operations inside the loop. Write your data file-processing loop inside your myscript.sh script file.\n\n\n\n\n\n\nTip\n\n\n\nWhen performing a destructive operation, it can be very helpful to view the actual commands that will be executed before doing them for real. To satisfy yourself that your script is coded correctly, prefix each command with echo so that it is simply printed in the terminal window. You will also want to either comment out I/O redirections or quote them, as they will otherwise affect the echo command. Here’s our practice loop:\nfor file in *.TXT; do\n    echo echo \"state,gender,year,firstname,count\" \"&gt;\" tempfile\n    echo cat $file \"&gt;&gt;\" tempfile\n    echo mv tempfile ${file/.TXT/.csv}\ndone\nWhen ready, remove the echo prefixes and remove quotes around redirection operators:\nfor file in *.TXT; do\n    echo \"state,gender,year,firstname,count\" &gt; tempfile\n    cat $file &gt;&gt; tempfile\n    mv tempfile ${file/.TXT/.csv}\ndone\n\n\n\nExercise\nAlice does not want to rename her files to .csv, she’s fine with them being named .TXT. And after learning that cat will concatenate multiple files given on the command line, she has what she thinks is a brilliant idea for performing the processing more simply. First she puts the common header record in a file:\necho \"state,gender,year,firstname,count\" &gt; header\nAnd then she writes this loop:\nfor file in *.TXT; do\n    cat header $file &gt; $file\ndone\nWell that turned out to be a disaster. What went wrong?"
  },
  {
    "objectID": "day2-bash-scripting.html#next-steps",
    "href": "day2-bash-scripting.html#next-steps",
    "title": "An introduction to Bash scripting",
    "section": "Next steps",
    "text": "Next steps\nThere’s a lot more to Bash scripting. Key next topics to study include:\n\nConditional statements\nProcessing command line arguments\nCreating scripts that can be invoked like built-in commands"
  },
  {
    "objectID": "day2-unices.html",
    "href": "day2-unices.html",
    "title": "Unices",
    "section": "",
    "text": "UNIX\n\nOriginally developed at AT&T Bell Labs circa 1970. Has experienced a long, multi-branched evolutionary path\n\nPOSIX (Portable Operating System Interface)\n\na set of specifications of what an OS needs to qualify as “a Unix”, to enhance interoperability among all the “Unix” variants\n\n\n\n\n\n\n\nThe unix family tree\n\n\n\n\nOS X\n\nis a Unix!\n\n\n\nLinux\n\nis not fully POSIX-compliant, but certainly can be regarded as functionally Unix\n\n\n\n\n\n\n\nSupports multi-users, multi-processes\nHighly modular: many small tools that do one thing well, and can be combined\nCulture of text files and streams\nPrimary OS on HPC (High Performance Computing Systems)\nMain OS on which Internet was built\n\n\n\n\n\n\n\n\nAll files have permissions and ownership.\n\n\n\nFile permissions"
  },
  {
    "objectID": "day2-unices.html#introduction-to-unix-and-its-siblings",
    "href": "day2-unices.html#introduction-to-unix-and-its-siblings",
    "title": "Unices",
    "section": "",
    "text": "UNIX\n\nOriginally developed at AT&T Bell Labs circa 1970. Has experienced a long, multi-branched evolutionary path\n\nPOSIX (Portable Operating System Interface)\n\na set of specifications of what an OS needs to qualify as “a Unix”, to enhance interoperability among all the “Unix” variants\n\n\n\n\n\n\n\nThe unix family tree\n\n\n\n\nOS X\n\nis a Unix!\n\n\n\nLinux\n\nis not fully POSIX-compliant, but certainly can be regarded as functionally Unix\n\n\n\n\n\n\n\nSupports multi-users, multi-processes\nHighly modular: many small tools that do one thing well, and can be combined\nCulture of text files and streams\nPrimary OS on HPC (High Performance Computing Systems)\nMain OS on which Internet was built\n\n\n\n\n\n\n\n\nAll files have permissions and ownership.\n\n\n\nFile permissions"
  },
  {
    "objectID": "day_4.html",
    "href": "day_4.html",
    "title": "Day 4 - Being reproducible to better collaborate: Tools and Practices",
    "section": "",
    "text": "Time (PST)\nActivity\n\n\n\n\n10:00am - 11:20am\nBuilding analytical workflows: meet targets (80 min)\n\n\n11:20am - 11:30am\nBreak (10 min)\n\n\n11:30am - 12:30am\nReproducible publication: meet rrtools (60 min)\n\n\n12:30pm - 1:30pm\nLunch (60 min)\n\n\n1:30pm - 2:20pm\nDocumenting & sharing things (50 min)\n\n\n2:20am - 2:30pm\nBreak (10 min)\n\n\n2:30pm - 4:00pm\nGroup Projects (90min)"
  },
  {
    "objectID": "vim.html",
    "href": "vim.html",
    "title": "Vim, the editor",
    "section": "",
    "text": "When you type almost any key, the corresponding character (letter, number, symbol, or whitespace) appears at the cursor\nCertain special keys do special operations (control, alt, command, super)\nFor a GUI app, you can/must click menus for other functionality"
  },
  {
    "objectID": "vim.html#your-typical-editor-works-like-this",
    "href": "vim.html#your-typical-editor-works-like-this",
    "title": "Vim, the editor",
    "section": "",
    "text": "When you type almost any key, the corresponding character (letter, number, symbol, or whitespace) appears at the cursor\nCertain special keys do special operations (control, alt, command, super)\nFor a GUI app, you can/must click menus for other functionality"
  },
  {
    "objectID": "vim.html#vim-has-modes",
    "href": "vim.html#vim-has-modes",
    "title": "Vim, the editor",
    "section": "Vim has modes",
    "text": "Vim has modes\n\nNormal\n\nfor almost everything except typing! Insert\n\n\nfor typing Visual\n\n\nfor highlighting a region to operate on\n\n\n(and others you don’t need to know about)"
  },
  {
    "objectID": "day1-pseudocode.html",
    "href": "day1-pseudocode.html",
    "title": "Flow charts and pseudocode",
    "section": "",
    "text": "Don’t start implementing nor coding without planning! It is important to stress that scientists write scripts to help them to investigate scientific question(s). Therefore scripting should not drive our analysis and thinking. We strongly recommend you take the time to plan ahead all the steps you need to conduct your analysis. Developing such a scientific workflow will help you to narrow down the tasks that are needed to move forward your analysis.\n\n\n\nCherubini et al., 2007"
  },
  {
    "objectID": "day1-pseudocode.html#planing-things",
    "href": "day1-pseudocode.html#planing-things",
    "title": "Flow charts and pseudocode",
    "section": "",
    "text": "Don’t start implementing nor coding without planning! It is important to stress that scientists write scripts to help them to investigate scientific question(s). Therefore scripting should not drive our analysis and thinking. We strongly recommend you take the time to plan ahead all the steps you need to conduct your analysis. Developing such a scientific workflow will help you to narrow down the tasks that are needed to move forward your analysis.\n\n\n\nCherubini et al., 2007"
  },
  {
    "objectID": "day1-pseudocode.html#flow-charts",
    "href": "day1-pseudocode.html#flow-charts",
    "title": "Flow charts and pseudocode",
    "section": "Flow charts",
    "text": "Flow charts\nFlowcharts are useful to visualize and develop analytical workflow. It guides planning and anticipating the various computing and analytical tasks that will be required to complete an analysis. It also helps explaining the different steps to your collaborators and team. You can use a flowchart to spell out the logic behind a analytical workflow before ever starting to code. It can help to organize big-picture thinking and provide a guide when it comes time to code. More specifically, flowcharts can:\n\nVisualize the sequence of the different phases of the analytical process from data collection to implementing analyses\nBetter define the scope and resources needed both in terms of project data management and computing resources needed\nDiscuss who will be in charge and involved in the development of the different parts of the workflow\nList the products / outputs that will result from your analysis - such as data, codes, publications, web presence, … - and discuss how to best preserve and share them\n\nThere are conventions on how to use symbols to represent different parts of a workflow (see here for example). Although it is good to be aware of those conventions for sharing a workflow with your community, within a team the most important aspect is to be coherent and stick to this usage overtime.\nThe main benefit for the project is the process and discussion as a team to develop the workflow. It will ensure that everybody is on the same page and has the opportunity to provide inputs on the project. This workflow should be updated regularly as the project evolves.\nA few workflow examples"
  },
  {
    "objectID": "day1-pseudocode.html#pseudocode",
    "href": "day1-pseudocode.html#pseudocode",
    "title": "Flow charts and pseudocode",
    "section": "Pseudocode",
    "text": "Pseudocode\nOften, programmers may write pseudocode as a next step, to provide greater detail than the flowchart in terms of processing steps and implementation. It will help you to define where iterations will be needed but also detect repeating blocks that might be well suited to be handled via the development of a function.\nThis technique aims at developing a sequence of pragrammatical steps in a manner that is easy to understand for anyone with basic programming knowledge. Pseudocode can be implemented more or less formally and at various levels of details. One additional advantage of going through this process is that it is agnostic of the tools / programming languages that you will be using to develop your analytical workflow.\nIn this course, we will be focusing on the process more than the exact syntax to use, keeping the level of details at a level that provide more details than a flow chart."
  },
  {
    "objectID": "day2-cli_practice.html",
    "href": "day2-cli_practice.html",
    "title": "Practicing CLI",
    "section": "",
    "text": "In the following code examples, you need to type the command, but not include the command prompt (e.g., brun@taylor:~$) which just shows that the computer is ready to accept a command.\nWe’ll start by:\n\ncreating two directories with mkdir (make directory)\ncreate a simple text file using echo\nShow the contents of that file using cat (concatenate)\n\nbrun@taylor:~$ mkdir cli_intro\nbrun@taylor:~$ mkdir cli_intro/data\nbrun@taylor:~$ echo \"# Tutorial files related to CLI\" &gt; cli_intro/README.md\nbrun@taylor:~$ cat cli_intro/README.md\n# Tutorial files related to CLI\nbrun@taylor:~$ \n\n\n\n\n\ncopy a file into your directory with cp (copy)\nchange our working directory to that newly created directory using cd (change directory)\nlist the files in the directory with ls (list)\nlook where we are in the filesystem using pwd (print working directory)\nget an overview of the directory contents using tree\n\nUpload files from your local machine to the server using the different techniques mentioned above, You can download the 10min-loop.R file to your local machine from https://aurora.nceas.ucsb.edu/~brun/10min-loop.R\n\n\n\n\ndownload the data to your laptop: https://aurora.nceas.ucsb.edu/~brun/sampledata.zip\nunzip the folder\nupload the folder to Taylor using scp or cyberduck\n\nYour should end up with something like this:\nbrun@taylor:~$ ls -l\ntotal 16\n-rw-r--r-- 1 brun esmdomainusers   90 Aug 25 05:36 10min-loop.R\ndrwxr-xr-x 3 brun esmdomainusers 4096 Aug 24 23:05 github\ndrwxr-xr-x 3 brun esmdomainusers 4096 Aug 16 05:00 R\ndrwxr-xr-x 2 brun esmdomainusers 4096 Aug 25 05:37 sampledata\n\n\n\n\nNow, let’s create two subdirectories in the data directory: mammals and plots\n\nmove using cp all the mammals files from the sampledata folder to the mammals subdirectory; hint: you can use the wildcard *\nmove using mv the other files files from the sampledata folder to the plots subdirectory\ndouble check it is done using cd and ls\nremove rm the sampledata directory; hint: rmdir can only remove empty directories\nbonus: add a text file data_listing.txt in the data folder that lists all the files in it"
  },
  {
    "objectID": "day2-cli_practice.html#files-and-directories",
    "href": "day2-cli_practice.html#files-and-directories",
    "title": "Practicing CLI",
    "section": "",
    "text": "In the following code examples, you need to type the command, but not include the command prompt (e.g., brun@taylor:~$) which just shows that the computer is ready to accept a command.\nWe’ll start by:\n\ncreating two directories with mkdir (make directory)\ncreate a simple text file using echo\nShow the contents of that file using cat (concatenate)\n\nbrun@taylor:~$ mkdir cli_intro\nbrun@taylor:~$ mkdir cli_intro/data\nbrun@taylor:~$ echo \"# Tutorial files related to CLI\" &gt; cli_intro/README.md\nbrun@taylor:~$ cat cli_intro/README.md\n# Tutorial files related to CLI\nbrun@taylor:~$ \n\n\n\n\n\ncopy a file into your directory with cp (copy)\nchange our working directory to that newly created directory using cd (change directory)\nlist the files in the directory with ls (list)\nlook where we are in the filesystem using pwd (print working directory)\nget an overview of the directory contents using tree\n\nUpload files from your local machine to the server using the different techniques mentioned above, You can download the 10min-loop.R file to your local machine from https://aurora.nceas.ucsb.edu/~brun/10min-loop.R\n\n\n\n\ndownload the data to your laptop: https://aurora.nceas.ucsb.edu/~brun/sampledata.zip\nunzip the folder\nupload the folder to Taylor using scp or cyberduck\n\nYour should end up with something like this:\nbrun@taylor:~$ ls -l\ntotal 16\n-rw-r--r-- 1 brun esmdomainusers   90 Aug 25 05:36 10min-loop.R\ndrwxr-xr-x 3 brun esmdomainusers 4096 Aug 24 23:05 github\ndrwxr-xr-x 3 brun esmdomainusers 4096 Aug 16 05:00 R\ndrwxr-xr-x 2 brun esmdomainusers 4096 Aug 25 05:37 sampledata\n\n\n\n\nNow, let’s create two subdirectories in the data directory: mammals and plots\n\nmove using cp all the mammals files from the sampledata folder to the mammals subdirectory; hint: you can use the wildcard *\nmove using mv the other files files from the sampledata folder to the plots subdirectory\ndouble check it is done using cd and ls\nremove rm the sampledata directory; hint: rmdir can only remove empty directories\nbonus: add a text file data_listing.txt in the data folder that lists all the files in it"
  },
  {
    "objectID": "day2-cli_practice.html#aknowledgements",
    "href": "day2-cli_practice.html#aknowledgements",
    "title": "Practicing CLI",
    "section": "Aknowledgements",
    "text": "Aknowledgements\nAdapted from Matt Jones, OSS 2017, https://github.com/NCEAS/oss-lessons"
  },
  {
    "objectID": "day4-analytical_workflows.html",
    "href": "day4-analytical_workflows.html",
    "title": "Reproducible Analytical Workflows",
    "section": "",
    "text": "Please remember that every workflow or framework is opinionated. This is also true for the tools out there that can help you to develop reproducible workflows. It is great practice to test them on a small project to see if hey could fit your project needs and way of working and collaborating. In this section we will go over a few R packages that could be of interest for your project.\n\n\n\n\n\n\n\nConceptual workflow model promoted by the tidyverse. Note the program box around the workflow and the iterative nature of the analytical process described. Source: R for Data Science https://r4ds.hadley.nz/intro#fig-ds-diagram\n\n\n\n\n\n\nThe targets R package can be very useful if you have a complex workflow that is built of many parts that take time to rerun. Target can detect and run only the strict necessary steps to rerun when a specific change has been done to the workflow.\nHere for more: https://books.ropensci.org/targets/\nOur practice example: https://github.com/EDS-214/eds214-handson-targets\n\n\n\nAlthough the pointblank package is mainly framed as a validating tool of the various part of your workflow, it provide a set of tools and great integration with the R Markdown ecosystem. It is also quite flexible making it possible to leverage this tool in a variety of project setups.\nhttps://github.com/rstudio/pointblank"
  },
  {
    "objectID": "day4-analytical_workflows.html#leveraging-existing-frameworks-for-reproducibility",
    "href": "day4-analytical_workflows.html#leveraging-existing-frameworks-for-reproducibility",
    "title": "Reproducible Analytical Workflows",
    "section": "",
    "text": "Please remember that every workflow or framework is opinionated. This is also true for the tools out there that can help you to develop reproducible workflows. It is great practice to test them on a small project to see if hey could fit your project needs and way of working and collaborating. In this section we will go over a few R packages that could be of interest for your project.\n\n\n\n\n\n\n\nConceptual workflow model promoted by the tidyverse. Note the program box around the workflow and the iterative nature of the analytical process described. Source: R for Data Science https://r4ds.hadley.nz/intro#fig-ds-diagram\n\n\n\n\n\n\nThe targets R package can be very useful if you have a complex workflow that is built of many parts that take time to rerun. Target can detect and run only the strict necessary steps to rerun when a specific change has been done to the workflow.\nHere for more: https://books.ropensci.org/targets/\nOur practice example: https://github.com/EDS-214/eds214-handson-targets\n\n\n\nAlthough the pointblank package is mainly framed as a validating tool of the various part of your workflow, it provide a set of tools and great integration with the R Markdown ecosystem. It is also quite flexible making it possible to leverage this tool in a variety of project setups.\nhttps://github.com/rstudio/pointblank"
  },
  {
    "objectID": "day4-projects_team.html",
    "href": "day4-projects_team.html",
    "title": "Managing data-driven projects as a team",
    "section": "",
    "text": "Our goal is to centralize the management of your files (data, codes, …). Try to avoid having data sets spread among laptops or other personal computers; this makes it difficult for other team members to redo a particular analysis and it can become difficult to know which version of the data was used for a specific analysis. We recommend asking your institution if there are servers or cloud services available to you and use those tools to centralize your data management. This will also make sure that all your collaborators will be able to access the same version of the data using the same path.\nThere are many tools out there to help with file management. Here are a few questions to ask your teammates when organizing your project:\n\nCan everybody have access to this tool? This should overrule the “best” tool =&gt; maximize adoption\nWhat team practices should you set on how to use these tools? Example: naming convention for files\nAllow flexibility – acknowledge the technological level varies among collaborators. Empower them by showing how to best use these tools rather than doing it for them!"
  },
  {
    "objectID": "day4-projects_team.html#managing-your-files-as-a-team",
    "href": "day4-projects_team.html#managing-your-files-as-a-team",
    "title": "Managing data-driven projects as a team",
    "section": "",
    "text": "Our goal is to centralize the management of your files (data, codes, …). Try to avoid having data sets spread among laptops or other personal computers; this makes it difficult for other team members to redo a particular analysis and it can become difficult to know which version of the data was used for a specific analysis. We recommend asking your institution if there are servers or cloud services available to you and use those tools to centralize your data management. This will also make sure that all your collaborators will be able to access the same version of the data using the same path.\nThere are many tools out there to help with file management. Here are a few questions to ask your teammates when organizing your project:\n\nCan everybody have access to this tool? This should overrule the “best” tool =&gt; maximize adoption\nWhat team practices should you set on how to use these tools? Example: naming convention for files\nAllow flexibility – acknowledge the technological level varies among collaborators. Empower them by showing how to best use these tools rather than doing it for them!"
  },
  {
    "objectID": "day4-projects_team.html#naming-things",
    "href": "day4-projects_team.html#naming-things",
    "title": "Managing data-driven projects as a team",
    "section": "Naming things",
    "text": "Naming things\n\nDevelop naming conventions for files and folder:\n\nAvoid spaces (use underscores or dashes)\nAvoid punctuation or special characters\nTry to leverage alphabetical order (e.g. start with dates: 2020-05-08)\nUse descriptive naming (lite metadata)\nUse folders to structure/organize content\nKeep it simple\nMake it programmatically useful:\n\nUseful to select files (Wildcard *, regular expression)\nBut don’t forget Humans need to read file names too!!\n\n\n\nQuizz\nWhy do you think the second option would be best?\n\nx &lt;- 9.81  #  gravitational acceleration\n\ngravity_acc &lt;- 9.81  #  gravitational acceleration\n\nWhich filename would be the most useful?\n\n06-2020-08-sensor2-plot1.csv\n2020-05-08_light-sensor-1_plot-1.csv\nMeasurement 1.csv\n2020-05-08-light-sensor-1-plot-2.csv\n2020-05-08-windSensor1-plot3.csv\n\nThe most important is to make it consistent!\nGood reference on this topic from Jenny Bryan (RStudio)."
  },
  {
    "objectID": "day4-projects_team.html#organizing-things",
    "href": "day4-projects_team.html#organizing-things",
    "title": "Managing data-driven projects as a team",
    "section": "Organizing things",
    "text": "Organizing things\nAs we discussed previously, it is good practice to encapsulate your project and repositories are a good unit to start with. In this section we will talk about the case when your data sets start to be numerous or large enough that it is not possible anymore to keep them in your repository. Generally when you reach this amount of data to deal with, it also means that your personal computer might not be the best computer to efficiently process those data sets. Thus using a remote server (on premise or in the cloud) might become necessary.\n\nCodes should be managed using version control (git and GitHub for this course)   The repository should be stored in your home folder and the management of the different contributions should be resolved using git and GitHub and not by sharing directly the scripts. The main reason is that git does already the work for us by tracking the changes between the various versions, but also tracking which collaborator has made those changes and when.\nData sets should should be centralized in a shared folder that is available to all   It is very rare in an Environmental Data Science project that you will discover all the data you need from the start. In addition, a lot of environmental data sets are time-series, and one year later might need to be updated as your project progresses. Our goal here is to setup ourselves in a way that will help us to avoid duplication of data. Every time you duplicate a data set (for example on your own laptop), there is a risk that at some point it will become its own version."
  },
  {
    "objectID": "day4-projects_team.html#code",
    "href": "day4-projects_team.html#code",
    "title": "Managing data-driven projects as a team",
    "section": "Code",
    "text": "Code\nVersion control systems have been originally designed to track changes by rows in small text files, in other words they are well suited to manage codes. It is thus recommended to use them to track changes that you and your collaborators are making to the various scripts of your project. Mote on how to best do this in the section below."
  },
  {
    "objectID": "day4-projects_team.html#scripting-languages",
    "href": "day4-projects_team.html#scripting-languages",
    "title": "Managing data-driven projects as a team",
    "section": "Scripting languages",
    "text": "Scripting languages\nCompared to other programming languages (such as C, fortran, …), scripting languages are not required to be compiled to be executable. One consequence is that, generally, scripts will execute more slowly than a compiled executable program, because they need an interpreter. However, the more natural language oriented syntax of scripts make them easier to learn and use. In addition, numerous libraries are available to streamline scientific analysis.\n\nStructure of a script\nA script can be divided into several main sections. Each scripting language has its own syntax and style, but these main components are generally accepted:\nFrom the top to the bottom of your script:\n\nSummary explaining the purpose of the script\nAttribution: authors, contributors, date of last update, contact info\nImport of external modules / packages\nConstant definitions (g = 9.81)\nFunction definitions (ideally respecting the order in which they are called)\nMain code calling the different functions\n\n\n\nA few programming practices that will help a long way\n\nComment your code. This will allow you to inform your collaborators (but also your future self!) about the tasks your script accomplishes\nUse variables and constants instead of repeating values in different places of the code. This will let you update those values more easily\nChoose descriptive names for your variables and functions, not generic ones. If you store a list of files, do not use x for the variable name, use instead files. Even better use input_files if you are listing the files you are importing.\nBe consistent in terms of style (input_files, inputFiles,…) used to name variables and functions. Just pick one and stick to it!\nkeep it simple, stupid (KISS). Do not create overly complicated or nested statements. Break your tasks in several simple lines of code instead of embedding a lot of executions in one (complicated line). It will save you time while debugging and make your code more readable to others\nGo modular! Break down tasks into small code fragments such as functions or code chunks. It will make your code reusable for you and others (if well documented). Keep functions simple; they should only implement one or few (related) tasks\nDon’t Repeat Yourself (DRY). If you start copy/pasting part of your code changing a few parameters =&gt; write a function and call it several times with different parameters. Add flow control such as loops and conditions. It will be easier to debug, change and maintain\nTest your code. Test your code against values you would expect or computed with another software. Try hedge cases, such as NA, negative values, ….\nIterate with small steps, implement few changes at a time to your code. Test, fix, and move forward!\n\nWe hope this overview section about scientific programming has raised your interest in learning more about best practices and tools for developing reproducible workflows using scripting languages.\n\n\nNotebooks and scripts\nWith the increasing popularity of notebooks in scientific projects, it can sometimes be confusing to know when to use one. The good news is that there is not really a wrong or right here and that actually both can be used in a complementary manner and each data scientist will have her/his preference. However here are a few tips to help you decide:\n\nKeep your Notebook at a length you will feel comfortable reading. It if starts to be a long it might be time to think about if some code could be move to another document (scripts or another notebook)\nScripts might be better suited for tasks you need to rerun frequently\nIf you have developed many functions for your analysis, it might be worth storing them in a script outside your main notebook"
  },
  {
    "objectID": "day4-projects_team.html#data",
    "href": "day4-projects_team.html#data",
    "title": "Managing data-driven projects as a team",
    "section": "Data",
    "text": "Data\nIt is recommended to keep the raw-data you are collecting separated from any data you might generate at various steps of your workflow. This will help you to trace back any problems, but also make your work more reproducible because you started your processing directly from the original data. There are different ways of ensuring this. A common one is to create a raw-data (sub)folder to store the data. You can even play with the file access settings to make those files read-only.\n\n\n\nhttps://github.com/benmarwick/rrtools"
  },
  {
    "objectID": "day_2.html",
    "href": "day_2.html",
    "title": "Day 2 - Working on a remote server",
    "section": "",
    "text": "Time (PST)\nActivity\n\n\n\n\n10:00am - 10:50am\nLecture 1: Working on a remote server Server (50 min)\n\n\n10:50am - 11:00am\nBreak (10 min)\n\n\n11:00am - 12:30am\nInteractive session 1: Command line Interface (50 min)\n\n\n12:30pm - 1:30pm\nLunch (60 min)\n\n\n1:30pm - 3:00pm\nHybrid: Automating things with bash (90min)\n\n\n3:00pm - 3:10pm\nBreak (10 min)\n\n\n3:10pm - 4:00pm\nCollaborating on a server (20 min) & Uploading things to a server (30 min)"
  },
  {
    "objectID": "day_5.html",
    "href": "day_5.html",
    "title": "Reproducible for all",
    "section": "",
    "text": "Time (PST)\nActivity\n\n\n\n\n10:00am - 10:30am\nOpen Source Software for Data Science (30 min) (30min)*\n\n\n10:30am - 12:30pm\nGroup project (120min)\n\n\n12:300pm - 1:30pm\nLunch \n\n\n1:30pm - 2:00pm\nMeet your next instructor – Kelly Caylor, Office of Research and Earth Research Institute, UCSB\n\n\n2:00pm - 3:00pm\nGroup project presentations - p1 (60 min).\n\n\n3:00pm - 3:10pm\nBreak 2 (10min)\n\n\n3:10pm - 3:40pm\nGroup project presentations - p2 (30 min)\n\n\n3:40pm - 4:00pm\nFinal thoughts and feedback\n\n\n\n\n\n\n\n\n\n\nMaterials\n\n\n\n\nSchedule"
  },
  {
    "objectID": "day_5.html#scope-and-debugging",
    "href": "day_5.html#scope-and-debugging",
    "title": "Reproducible for all",
    "section": "",
    "text": "Materials"
  },
  {
    "objectID": "day_5.html#group-projects-presentations",
    "href": "day_5.html#group-projects-presentations",
    "title": "Reproducible for all",
    "section": "",
    "text": "Schedule"
  },
  {
    "objectID": "day1-github_commit_messages.html",
    "href": "day1-github_commit_messages.html",
    "title": "On good commit messages",
    "section": "",
    "text": "https://xkcd.com/1296/\nCommit messages are critical for others, including your future self, to understand the motivation behind the changes that were implemented. Combined into the history of your file and repository, those messages help to understand the content and its evolution over time\nSo what is a good commit message? Well I think the answer actually starts with what is a good commit? Which actually begins when you are choosing how to group files during the staging.\nA good commit:\nA good commit message:\nNote: you can add a body to a commit message if you want to describe the content in greater details, but keep the first line (message) separated."
  },
  {
    "objectID": "day1-github_commit_messages.html#references",
    "href": "day1-github_commit_messages.html#references",
    "title": "On good commit messages",
    "section": "References",
    "text": "References\n\nHow to Write a Git Commit Message: https://chris.beams.io/posts/git-commit/"
  },
  {
    "objectID": "day1-hands-on_drawings.html",
    "href": "day1-hands-on_drawings.html",
    "title": "Hands-on: Planning your work",
    "section": "",
    "text": "Draw the workflow to combine 4 datasets about stream flow and water chemistry in a way that will let you investigate the impact of the 1989 Hurricane Hugo on Stream Chemistry in the Luquillo Mountains of Puerto Rico\n\n\n\nTable structures are different =&gt; only some variables / columns overlap among the different sites\nUnits used among the various sites are different\nPeriod covered is not perfectly aligned for all the time-series. Some sites start or end before others, but there is a period of overlap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to be able to produce this plot"
  },
  {
    "objectID": "day1-hands-on_drawings.html#goal",
    "href": "day1-hands-on_drawings.html#goal",
    "title": "Hands-on: Planning your work",
    "section": "",
    "text": "Draw the workflow to combine 4 datasets about stream flow and water chemistry in a way that will let you investigate the impact of the 1989 Hurricane Hugo on Stream Chemistry in the Luquillo Mountains of Puerto Rico\n\n\n\nTable structures are different =&gt; only some variables / columns overlap among the different sites\nUnits used among the various sites are different\nPeriod covered is not perfectly aligned for all the time-series. Some sites start or end before others, but there is a period of overlap\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe want to be able to produce this plot"
  },
  {
    "objectID": "day1-hands-on_drawings.html#remember",
    "href": "day1-hands-on_drawings.html#remember",
    "title": "Hands-on: Planning your work",
    "section": "Remember",
    "text": "Remember\n\nEach node represents a step or an input/output\nEach connecting edge represents data flow or processing\n\n\n\nOh wait…\nActually there is more"
  },
  {
    "objectID": "day1-hands-on_drawings.html#references",
    "href": "day1-hands-on_drawings.html#references",
    "title": "Hands-on: Planning your work",
    "section": "References",
    "text": "References\nExercise based on: Schaefer, D., McDowell, W., Scatena, F., & Asbury, C. (2000). Effects of hurricane disturbance on stream water concentrations and fluxes in eight tropical forest watersheds of the Luquillo Experimental Forest, Puerto Rico. Journal of Tropical Ecology, 16(2), 189-207. doi:10.1017/S0266467400001358\nData available here: McDowell, W. 2021. Chemistry of stream water from the Luquillo Mountains ver 4923052. Environmental Data Initiative. https://doi.org/10.6073/pasta/ddb4d61aea2fb18b77540e9b0c424684 (Accessed 2021-08-06)."
  },
  {
    "objectID": "group_project.html",
    "href": "group_project.html",
    "title": "Group Project",
    "section": "",
    "text": "We are going to build on the exercise we used the first day and work on implementing and refining the workflow we developed during that session.\n\n\nBy group of 4-5 collaborators:\n\nFind an awesome name for your group (one of the hardest steps)\nSetup a shared GitHub repository\nUse the MEDS server Taylor as your main computing resource\nUse GitHub to manage your code development in a collaborative manner\nUse shared folder on Taylor to manage your data /courses/EDS214/group_project/my_group_name\nDocument your work as you go!!\n\nComment your code\nAdd project details to your README, including reference to data sources\n\nUse GitHub issues to track your work and discuss progress and tasks\n\n\n\n\n\n\n\nThe Luquillo Experimental Forest (LEF) has been a center of tropical forestry research for nearly a century. In addition, the LEF is a recreation site for over a half a million people per year, a water supply for approximately 20% of Puerto Rico’s population, a regional center for electronic communication, and a refuge of Caribbean biodiversity. It is the goal of the USDA Forest Service and the University of Puerto to promote and maintain the forest’s role as a center of active and dynamic scientific inquiry. However, to maintain the ecological integrity of the forest while balancing the many demands placed upon it’s resources, certain protocol is required. This guide provides the major protocols that govern research in the LEF. These protocols are designed to help researchers protect the forests, obey the law, create an amiable and non-discriminatory work environment, and provide a historical record for future scientists\nHere for more information about the Luquillo site: https://lternet.edu/site/luquillo-lter/\n\n\n\n\n\n\n\n\n\n\n\n\n\nCan you recreate that plot (content, not style wise… you can do way better 🙂)\n\nWrite a quarto document that: - reads the data in (from a local copy on Taylor) - processes the data as needed to be plotted - use ggplot to create a similar (but better looking) plat - comment your code and use markdown text to provide context to your code - render your report as html\n\nAsk your own question! You are encouraged to define you own question around this topic as a team\n\nGo wild!! Ask your own scientific question using the stream chemistry data but also any other external source of data (like historical hurricane tracks from NOAA). The focus in this part of the project is on the process of how you will be answering the question (planning, workflow, code, data, ….), not on the results per se\n\n\n\nData are available from the Environmental Data Initiative (EDI) that is hosting most of the data of the Long Term Ecological Research (LTER) Network.\n\n\nMcDowell, W. and International Institute of Tropical Forestry(IITF), USDA Forest Service. 2022. Chemistry of stream water from the Luquillo Mountains ver 4923061. Environmental Data Initiative. https://doi.org/10.6073/pasta/570231c2807a6396ced6a89ef7547bd4 (Accessed 2023-08-24).\n\n\n\n\nMajor points to hit:\n\nGoal/Question + workflow you used to achieve it (aka THE plan)\nHow did you set up your project (server, data, code, …)\nHow did you organize your team (tasks, who did what)\nResults\nMain challenges\n\n\n\n\n\n\n\ngroup_name\ngithub_repo\nslides\n\n\n\n\nA1\nhttps://github.com/katleyq/A1_GroupProject.git\nhttps://docs.google.com/presentation/d/1MIxRjgidlx6aNrJP_j3NrdGwueftTrKvZMohyeHCO5I/edit?usp=sharing\n\n\ngot git?\nhttps://github.com/BenVerst/Got-Git\nhttps://docs.google.com/presentation/d/1Xy1bYJpYrcdP7mRlN-8UuHn8JdGMpaXX08gWgcHOIzo/edit#slide=id.g27a90bb0dba_0_91\n\n\nThe Brenniacs\nhttps://github.com/oksanaprotsukha/thebrenniacs\nhttps://docs.google.com/presentation/d/1Xy1bYJpYrcdP7mRlN-8UuHn8JdGMpaXX08gWgcHOIzo/edit#slide=id.g27a90bb0dba_0_91\n\n\nGroup Fourk\nhttps://github.com/hmchilders/EDS214_Group4\nhttps://docs.google.com/presentation/d/1-Vsyrfgm5CBWHuNI8qwxv7C0wU5s8nVdVAZRFmTTEZE/edit?usp=sharing\n\n\nTropic-Thunder\nhttps://github.com/saingersoll/tropic-thunder\nhttps://docs.google.com/presentation/d/1548heJAcJN9x8lYXY6uKkh3X2imWDLK9GxKLMNkqnvU/edit?usp=sharing\n\n\nTeam Siguana\nhttps://github.com/Vanessa-Salgado/EDS214_group_project_team_siguanaa\nhttps://docs.google.com/presentation/d/15Ea1zf39oQlQBq6mOkdQZDI8MJw-MjuEHHdVUFA5Rxw/edit#slide=id.g2798449c7d5_0_10\n\n\nSMOL\nhttps://github.com/olleholt/SMOL\nhttps://docs.google.com/presentation/d/1kK2p3mnl_-ar3ZZahE1P1WqYCdvmFPik9CIJAOHSEVE/edit?pli=1#slide=id.p\n\n\n\n\n\n\nSchaefer, D., McDowell, W., Scatena, F., & Asbury, C. (2000). Effects of hurricane disturbance on stream water concentrations and fluxes in eight tropical forest watersheds of the Luquillo Experimental Forest, Puerto Rico. Journal of Tropical Ecology, 16(2), 189-207. doi:10.1017/S0266467400001358"
  },
  {
    "objectID": "group_project.html#here-are-the-expectations",
    "href": "group_project.html#here-are-the-expectations",
    "title": "Group Project",
    "section": "",
    "text": "By group of 4-5 collaborators:\n\nFind an awesome name for your group (one of the hardest steps)\nSetup a shared GitHub repository\nUse the MEDS server Taylor as your main computing resource\nUse GitHub to manage your code development in a collaborative manner\nUse shared folder on Taylor to manage your data /courses/EDS214/group_project/my_group_name\nDocument your work as you go!!\n\nComment your code\nAdd project details to your README, including reference to data sources\n\nUse GitHub issues to track your work and discuss progress and tasks"
  },
  {
    "objectID": "group_project.html#background",
    "href": "group_project.html#background",
    "title": "Group Project",
    "section": "",
    "text": "The Luquillo Experimental Forest (LEF) has been a center of tropical forestry research for nearly a century. In addition, the LEF is a recreation site for over a half a million people per year, a water supply for approximately 20% of Puerto Rico’s population, a regional center for electronic communication, and a refuge of Caribbean biodiversity. It is the goal of the USDA Forest Service and the University of Puerto to promote and maintain the forest’s role as a center of active and dynamic scientific inquiry. However, to maintain the ecological integrity of the forest while balancing the many demands placed upon it’s resources, certain protocol is required. This guide provides the major protocols that govern research in the LEF. These protocols are designed to help researchers protect the forests, obey the law, create an amiable and non-discriminatory work environment, and provide a historical record for future scientists\nHere for more information about the Luquillo site: https://lternet.edu/site/luquillo-lter/"
  },
  {
    "objectID": "group_project.html#goals",
    "href": "group_project.html#goals",
    "title": "Group Project",
    "section": "",
    "text": "Can you recreate that plot (content, not style wise… you can do way better 🙂)\n\nWrite a quarto document that: - reads the data in (from a local copy on Taylor) - processes the data as needed to be plotted - use ggplot to create a similar (but better looking) plat - comment your code and use markdown text to provide context to your code - render your report as html\n\nAsk your own question! You are encouraged to define you own question around this topic as a team\n\nGo wild!! Ask your own scientific question using the stream chemistry data but also any other external source of data (like historical hurricane tracks from NOAA). The focus in this part of the project is on the process of how you will be answering the question (planning, workflow, code, data, ….), not on the results per se"
  },
  {
    "objectID": "group_project.html#data",
    "href": "group_project.html#data",
    "title": "Group Project",
    "section": "",
    "text": "Data are available from the Environmental Data Initiative (EDI) that is hosting most of the data of the Long Term Ecological Research (LTER) Network.\n\n\nMcDowell, W. and International Institute of Tropical Forestry(IITF), USDA Forest Service. 2022. Chemistry of stream water from the Luquillo Mountains ver 4923061. Environmental Data Initiative. https://doi.org/10.6073/pasta/570231c2807a6396ced6a89ef7547bd4 (Accessed 2023-08-24)."
  },
  {
    "objectID": "group_project.html#group-project-presentation-10min-5-slides",
    "href": "group_project.html#group-project-presentation-10min-5-slides",
    "title": "Group Project",
    "section": "",
    "text": "Major points to hit:\n\nGoal/Question + workflow you used to achieve it (aka THE plan)\nHow did you set up your project (server, data, code, …)\nHow did you organize your team (tasks, who did what)\nResults\nMain challenges"
  },
  {
    "objectID": "group_project.html#groups-2023",
    "href": "group_project.html#groups-2023",
    "title": "Group Project",
    "section": "",
    "text": "group_name\ngithub_repo\nslides\n\n\n\n\nA1\nhttps://github.com/katleyq/A1_GroupProject.git\nhttps://docs.google.com/presentation/d/1MIxRjgidlx6aNrJP_j3NrdGwueftTrKvZMohyeHCO5I/edit?usp=sharing\n\n\ngot git?\nhttps://github.com/BenVerst/Got-Git\nhttps://docs.google.com/presentation/d/1Xy1bYJpYrcdP7mRlN-8UuHn8JdGMpaXX08gWgcHOIzo/edit#slide=id.g27a90bb0dba_0_91\n\n\nThe Brenniacs\nhttps://github.com/oksanaprotsukha/thebrenniacs\nhttps://docs.google.com/presentation/d/1Xy1bYJpYrcdP7mRlN-8UuHn8JdGMpaXX08gWgcHOIzo/edit#slide=id.g27a90bb0dba_0_91\n\n\nGroup Fourk\nhttps://github.com/hmchilders/EDS214_Group4\nhttps://docs.google.com/presentation/d/1-Vsyrfgm5CBWHuNI8qwxv7C0wU5s8nVdVAZRFmTTEZE/edit?usp=sharing\n\n\nTropic-Thunder\nhttps://github.com/saingersoll/tropic-thunder\nhttps://docs.google.com/presentation/d/1548heJAcJN9x8lYXY6uKkh3X2imWDLK9GxKLMNkqnvU/edit?usp=sharing\n\n\nTeam Siguana\nhttps://github.com/Vanessa-Salgado/EDS214_group_project_team_siguanaa\nhttps://docs.google.com/presentation/d/15Ea1zf39oQlQBq6mOkdQZDI8MJw-MjuEHHdVUFA5Rxw/edit#slide=id.g2798449c7d5_0_10\n\n\nSMOL\nhttps://github.com/olleholt/SMOL\nhttps://docs.google.com/presentation/d/1kK2p3mnl_-ar3ZZahE1P1WqYCdvmFPik9CIJAOHSEVE/edit?pli=1#slide=id.p"
  },
  {
    "objectID": "group_project.html#reference",
    "href": "group_project.html#reference",
    "title": "Group Project",
    "section": "",
    "text": "Schaefer, D., McDowell, W., Scatena, F., & Asbury, C. (2000). Effects of hurricane disturbance on stream water concentrations and fluxes in eight tropical forest watersheds of the Luquillo Experimental Forest, Puerto Rico. Journal of Tropical Ecology, 16(2), 189-207. doi:10.1017/S0266467400001358"
  },
  {
    "objectID": "day4-reproducible_publications.html",
    "href": "day4-reproducible_publications.html",
    "title": "Reproducible Publication with rrtools",
    "section": "",
    "text": "A great overview of this approach to reproducible papers comes from:\nThis lesson will draw from existing materials:\nThe key idea in Marwick et al. (2018) is that of the research compendium: A single container for not just the journal article associated with your research but also the underlying analysis, data, and even the required software environment required to reproduce your work. Research compendia make it easy for researchers to do their work but also for others to inspect or even reproduce the work because all necessary materials are readily at hand due to being kept in one place.\nRather than a constrained set of rules, the research compendium is a scaffold upon which to conduct reproducible research using open science tools such as:\nFortunately for us, Ben Marwick (and others) have written an R package called rrtools that helps us create a research compendium from scratch.\nTo start a reproducible paper with rrtools, run:\n# Install the package\nremotes::install_github(\"benmarwick/rrtools\")\n\n# Attach the library\nlibrary(rrtools)\n\n# Create the compendium skeleton\nuse_compendium(\"mypaper\")\nYou should see output similar to the below:\nrrtools has created the beginnings of a research compendium for us. At this point, it looks mostly the same as an R package. That’s because it uses the same underlying folder structure and metadata and therefore it technically is an R package. And this means our research compendium will be easy to install, just like an R package.\nBefore we get to writing our reproducible paper, let’s fill in some more structure. Let’s:\nusethis::use_apl2_license() # Change this\nrrtools::use_readme_rmd()\nrrtools::use_analysis()\nAt this point, we’re ready to start writing the paper. To follow the structure rrtools has put in place for us, here are some pointers:\nIt would also be a good idea to initialize this folder as a git repo for maximum reproducibility:\nusethis::use_git()\nAfter that, push a copy up to GitHub.\n## create github repository and configure as git remote\nusethis::use_github()\nHopefully, now that you’ve created a research compendium with rrtools, you can imagine how a pre-defined structure like the one rrtools creates might help you organize your reproducible research and also make it easier for others to understand your work.\nFor a more complete example than the one we built above, take a look at benmarwick/teaching-replication-in-archaeology."
  },
  {
    "objectID": "day4-reproducible_publications.html#aknowledgement",
    "href": "day4-reproducible_publications.html#aknowledgement",
    "title": "Reproducible Publication with rrtools",
    "section": "Aknowledgement",
    "text": "Aknowledgement\nThe rrtools example had been ported from NCEAS Reproducible Research Techniques for Synthesis"
  },
  {
    "objectID": "rstudio_debugging.html",
    "href": "rstudio_debugging.html",
    "title": "Debugging in RStudio",
    "section": "",
    "text": "All variables in a program may not be accessible at all locations in that program. This depends on where you have declared a variable.\nVariables that are defined inside a function body have a local scope, and those defined outside have a global scope.\nThis means that local variables can be only be accessed inside the function in which they are declared. Thus, any ordinary assignments done within the function are local and temporary and are lost after exit from the function; whereas global variables can be accessed throughout the program body by all functions.\n\n\nTry to predict what will be outputted by the following:\n\nfoo &lt;- function() {\n    bar &lt;- 1\n    return(bar)\n}\n\n\nbar &lt;- 2\nfoo &lt;- function() {\n    bar &lt;- 1\n    return(bar)\n}\n\n\nfoo &lt;- function() {\n    bar &lt;&lt;- 1  #hein?!\n    return(bar)\n}\n\nAlthough &lt;&lt;- is a specificity of R, the general concept of global and local scope is valid for Python and others.\n\n\n\n\nfoo &lt;- function() {\n    bar &lt;- bar + 1\n    return(bar)\n}"
  },
  {
    "objectID": "rstudio_debugging.html#scope-global-vs.-local-environment",
    "href": "rstudio_debugging.html#scope-global-vs.-local-environment",
    "title": "Debugging in RStudio",
    "section": "",
    "text": "All variables in a program may not be accessible at all locations in that program. This depends on where you have declared a variable.\nVariables that are defined inside a function body have a local scope, and those defined outside have a global scope.\nThis means that local variables can be only be accessed inside the function in which they are declared. Thus, any ordinary assignments done within the function are local and temporary and are lost after exit from the function; whereas global variables can be accessed throughout the program body by all functions.\n\n\nTry to predict what will be outputted by the following:\n\nfoo &lt;- function() {\n    bar &lt;- 1\n    return(bar)\n}\n\n\nbar &lt;- 2\nfoo &lt;- function() {\n    bar &lt;- 1\n    return(bar)\n}\n\n\nfoo &lt;- function() {\n    bar &lt;&lt;- 1  #hein?!\n    return(bar)\n}\n\nAlthough &lt;&lt;- is a specificity of R, the general concept of global and local scope is valid for Python and others.\n\n\n\n\nfoo &lt;- function() {\n    bar &lt;- bar + 1\n    return(bar)\n}"
  },
  {
    "objectID": "rstudio_debugging.html#debugging",
    "href": "rstudio_debugging.html#debugging",
    "title": "Debugging in RStudio",
    "section": "Debugging",
    "text": "Debugging\nThis section is adapted from the more detailed material available on the RStudio website: https://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio\nDebugging is a broad topic that can be approached in many ways. Generically, at some point you will likely attempt to execute a script in R, receive errors and not know exactly what caused the errors. One approach would be to run your code line by line, but RStudio has some useful built-in debugging features.\nOne basic approach to debugging is to create a breakpoint in your code – this forces your code to “stop” executing when it reaches some certain function or line number in your code, allowing you then to examine the state of various variables, etc. The easiest way to do this is to set the breakpoint by manually clicking next to the desired line number in the code panel, per this web example:\n\n\n\nhttps://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio#stopping-on-a-line\n\n\nSetting this editor breakpoint creates tracing code associated with the R function object. You can remove the breakpoint by clicking on the red dot by the line number. Also note the Debug toolbar has an option to clear all breakpoints.\nNote: keep in mind that you can’t set breakpoints anywhere. In general, you want to insert breakpoints at top-level expressions or simple, named functions.\nAn alternative way to set breakpoints is with the browser() function. This must be typed into your code, per this web example:\n\n\n\nhttps://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio\n\n\n(image source: https://support.rstudio.com/hc/en-us/articles/205612627-Debugging-with-RStudio#stopping-on-a-line)\n\nThe debugging interface\nOnce your code hits a breakpoint, RStudio enters debugging mode. Details on the debugging interface can be found here, but we summarize the main points below:\nThe Environment tab will display the objects in the environment of the currently executing function (i.e., the function’s defined arguments)\nThe Traceback literally traces back how you arrived at the currently executing function (latest executed command is at the top of the list). This is analagous to the “call stack” in other programming languages.\nThe Code window highlights the currently executing function and may create a new tab, named Source Viewer, when the current function the debugger is stepping through is not in the main R script.\nThe Console retains most of its normal functionality in debugging mode, but contains some additional buttons that appear at the top to facilitate moving through code lines (see below).\n\n\n\nDebugging buttons on the console\n\n\n\n\nChallenge\n\nStart a new R Script\nWrite a function that computes the percentage of a number: n*p/100\nAdd a ratio factor as an argument so we can also use your function to compute 1/1000\nAdd a break point inside the function and use it to inspect local variables of your function"
  },
  {
    "objectID": "rstudio_debugging.html#further-reading",
    "href": "rstudio_debugging.html#further-reading",
    "title": "Debugging in RStudio",
    "section": "Further Reading",
    "text": "Further Reading\n\nFunctional programming in R: http://adv-r.had.co.nz/Functional-programming.html#functional-programming\nScoping in R: http://adv-r.had.co.nz/Functions.html"
  },
  {
    "objectID": "day4-sharing_things.html",
    "href": "day4-sharing_things.html",
    "title": "Sharing Things",
    "section": "",
    "text": "You have been using it all week!! and before!! All the course material has been developed as an R Markdown based website, distill website to be precise (see here for more), and it is also a great way to publish and share your note book with your team and the broader community. Also checkout Quarto that is a cross-language tool to render documents from code!"
  },
  {
    "objectID": "day4-sharing_things.html#github-pages-the-r-markdown-quarto-family",
    "href": "day4-sharing_things.html#github-pages-the-r-markdown-quarto-family",
    "title": "Sharing Things",
    "section": "",
    "text": "You have been using it all week!! and before!! All the course material has been developed as an R Markdown based website, distill website to be precise (see here for more), and it is also a great way to publish and share your note book with your team and the broader community. Also checkout Quarto that is a cross-language tool to render documents from code!"
  },
  {
    "objectID": "day4-sharing_things.html#interactive-web-applications",
    "href": "day4-sharing_things.html#interactive-web-applications",
    "title": "Sharing Things",
    "section": "Interactive web applications",
    "text": "Interactive web applications\nWhen you have complex data that you want to not only visualize but also to let the user interact with your data or customize parameters used in an analysis, interactive web applications are a great way to increase engagement. The good news is that you do not need to be a web developer anymore to spin such applications or dashboards.\n\nR Shiny \nR Shiny is a very interesting framework that lets you write R code that will then be translated into javascript for you and thus let you develop web application without having to learn any new programming language. Note that you will need a server to host the application.\nCheck out this gallery of shiny apps: https://shiny.rstudio.com/gallery/\nGetting started with Shiny: https://shiny.rstudio.com/tutorial/\n\n\nHtml widgets\nHtml widgets offer some great interactive data visualization. It is more limited that Shiny because you can not modify parameters to modify the data use, but it has the advantage that it does not need a server to run the widget and it can be inserted directly into an R Markdown.\nHere to get started: https://www.htmlwidgets.org/"
  },
  {
    "objectID": "day4-sharing_things.html#plotly",
    "href": "day4-sharing_things.html#plotly",
    "title": "Sharing Things",
    "section": "plotly ",
    "text": "plotly \nplotly enable you to develop great interactive data visualizations. It has the advantage to be available both in R and Python (and javascript). One great thing in R is that if you are a ggplot master, you can write your plot code using ggplot and transform it into a plotly plot with one line of code (here for an example)\nNote there are also other python libraries to create interactive plots, here are a few: https://mode.com/blog/python-interactive-plot-libraries/"
  },
  {
    "objectID": "day4-sharing_things.html#binder-jupyter",
    "href": "day4-sharing_things.html#binder-jupyter",
    "title": "Sharing Things",
    "section": "Binder & jupyter ",
    "text": "Binder & jupyter \nTransform your git based repo into an interactive jupyter notebook https://mybinder.org/!! So other researchers can run your code without having to install anything!\n\nTry it: https://mybinder.org/"
  },
  {
    "objectID": "day4-sharing_things.html#citing-your-code",
    "href": "day4-sharing_things.html#citing-your-code",
    "title": "Sharing Things",
    "section": "Citing your code",
    "text": "Citing your code\nNote that it is also possible to assign a DOI to cite a specific version of your repository. For example, see here for more information on how to link Zenodo and GitHub."
  },
  {
    "objectID": "day4-sharing_things.html#session-info",
    "href": "day4-sharing_things.html#session-info",
    "title": "Sharing Things",
    "section": "Session info",
    "text": "Session info\nYour analysis was done with specific versions both of the program used but also of all the packages involved, as well as the specifications of Operating System (OS) that was used. The good use is that there ar tools to let you capture this information in a systematic manner.\n\nsessionInfo()\n\nR version 4.3.1 (2023-06-16)\nPlatform: x86_64-pc-linux-gnu (64-bit)\nRunning under: Ubuntu 22.04.4 LTS\n\nMatrix products: default\nBLAS:   /usr/lib/x86_64-linux-gnu/openblas-pthread/libblas.so.3 \nLAPACK: /usr/lib/x86_64-linux-gnu/openblas-pthread/libopenblasp-r0.3.20.so;  LAPACK version 3.10.0\n\nlocale:\n [1] LC_CTYPE=C.UTF-8       LC_NUMERIC=C           LC_TIME=C.UTF-8       \n [4] LC_COLLATE=C.UTF-8     LC_MONETARY=C.UTF-8    LC_MESSAGES=C.UTF-8   \n [7] LC_PAPER=C.UTF-8       LC_NAME=C              LC_ADDRESS=C          \n[10] LC_TELEPHONE=C         LC_MEASUREMENT=C.UTF-8 LC_IDENTIFICATION=C   \n\ntime zone: UTC\ntzcode source: system (glibc)\n\nattached base packages:\n[1] stats     graphics  grDevices utils     datasets  methods   base     \n\nloaded via a namespace (and not attached):\n [1] htmlwidgets_1.6.4 compiler_4.3.1    fastmap_1.2.0     cli_3.6.3        \n [5] tools_4.3.1       htmltools_0.5.8.1 yaml_2.3.10       rmarkdown_2.27   \n [9] knitr_1.48        jsonlite_1.8.8    xfun_0.46         digest_0.6.36    \n[13] rlang_1.1.4       evaluate_0.24.0  \n\n\nor even better:\n\ndevtools::session_info()\n\n─ Session info ───────────────────────────────────────────────────────────────\n setting  value\n version  R version 4.3.1 (2023-06-16)\n os       Ubuntu 22.04.4 LTS\n system   x86_64, linux-gnu\n ui       X11\n language (EN)\n collate  C.UTF-8\n ctype    C.UTF-8\n tz       UTC\n date     2024-08-01\n pandoc   2.9.2.1 @ /usr/bin/ (via rmarkdown)\n\n─ Packages ───────────────────────────────────────────────────────────────────\n package     * version date (UTC) lib source\n cachem        1.1.0   2024-05-16 [1] CRAN (R 4.3.1)\n cli           3.6.3   2024-06-21 [1] CRAN (R 4.3.1)\n devtools      2.4.5   2022-10-11 [1] any (@2.4.5)\n digest        0.6.36  2024-06-23 [1] CRAN (R 4.3.1)\n ellipsis      0.3.2   2021-04-29 [1] CRAN (R 4.3.1)\n evaluate      0.24.0  2024-06-10 [1] CRAN (R 4.3.1)\n fastmap       1.2.0   2024-05-15 [1] CRAN (R 4.3.1)\n fs            1.6.4   2024-04-25 [1] CRAN (R 4.3.1)\n glue          1.7.0   2024-01-09 [1] CRAN (R 4.3.1)\n htmltools     0.5.8.1 2024-04-04 [1] CRAN (R 4.3.1)\n htmlwidgets   1.6.4   2023-12-06 [1] CRAN (R 4.3.1)\n httpuv        1.6.15  2024-03-26 [1] CRAN (R 4.3.1)\n jsonlite      1.8.8   2023-12-04 [1] CRAN (R 4.3.1)\n knitr         1.48    2024-07-07 [1] CRAN (R 4.3.1)\n later         1.3.2   2023-12-06 [1] CRAN (R 4.3.1)\n lifecycle     1.0.4   2023-11-07 [1] CRAN (R 4.3.1)\n magrittr      2.0.3   2022-03-30 [1] CRAN (R 4.3.1)\n memoise       2.0.1   2021-11-26 [1] CRAN (R 4.3.1)\n mime          0.12    2021-09-28 [1] CRAN (R 4.3.1)\n miniUI        0.1.1.1 2018-05-18 [1] CRAN (R 4.3.1)\n pkgbuild      1.4.4   2024-03-17 [1] CRAN (R 4.3.1)\n pkgload       1.4.0   2024-06-28 [1] CRAN (R 4.3.1)\n profvis       0.3.8   2023-05-02 [1] CRAN (R 4.3.1)\n promises      1.3.0   2024-04-05 [1] CRAN (R 4.3.1)\n purrr         1.0.2   2023-08-10 [1] CRAN (R 4.3.1)\n R6            2.5.1   2021-08-19 [1] CRAN (R 4.3.1)\n Rcpp          1.0.13  2024-07-17 [1] CRAN (R 4.3.1)\n remotes       2.5.0   2024-03-17 [1] CRAN (R 4.3.1)\n rlang         1.1.4   2024-06-04 [1] CRAN (R 4.3.1)\n rmarkdown     2.27    2024-05-17 [1] any (@2.27)\n sessioninfo   1.2.2   2021-12-06 [1] CRAN (R 4.3.1)\n shiny         1.9.1   2024-08-01 [1] CRAN (R 4.3.1)\n stringi       1.8.4   2024-05-06 [1] CRAN (R 4.3.1)\n stringr       1.5.1   2023-11-14 [1] CRAN (R 4.3.1)\n urlchecker    1.0.1   2021-11-30 [1] CRAN (R 4.3.1)\n usethis       3.0.0   2024-07-29 [1] CRAN (R 4.3.1)\n vctrs         0.6.5   2023-12-01 [1] CRAN (R 4.3.1)\n xfun          0.46    2024-07-18 [1] CRAN (R 4.3.1)\n xtable        1.8-4   2019-04-21 [1] CRAN (R 4.3.1)\n yaml          2.3.10  2024-07-26 [1] CRAN (R 4.3.1)\n\n [1] /home/runner/work/_temp/Library\n [2] /opt/R/4.3.1/lib/R/site-library\n [3] /opt/R/4.3.1/lib/R/library\n\n──────────────────────────────────────────────────────────────────────────────\n\n\nYou can save all this content to an session_info.txt file and upload it to your repository.\nIn python, using pip freeze &gt; requirements.txt or conda list --export &gt; requirements.txt will create a text file listing all the libraries (and their versions) used in a specific python environment. You can actually use this file to (re)install all the packages and specific versions into a new python environment. It is also great practice to add this file to your repository."
  },
  {
    "objectID": "day4-sharing_things.html#containers",
    "href": "day4-sharing_things.html#containers",
    "title": "Sharing Things",
    "section": "Containers",
    "text": "Containers\nA helpful abstraction for capturing the computing environment is a container, whereby a container is created from a set of instructions in a recipe. For the most common containerisation software, Docker, this recipe is called a Dockerfile. Docker is an open platform for developing, shipping, and running applications. Docker enables you to separate your applications from your infrastructure and ship the containers to others. A Docker container can be seen as a computer inside your computer.\n\n\n\n\n\nhttp://jsta.github.io/r-docker-tutorial/\n\n\n\n\nA few good readings:\n\nDocker for scientific reproducibility: https://journals.plos.org/ploscompbiol/article?id=10.1371/journal.pcbi.1008316\nThe Whole Tale project: combining containers with data repositories https://wholetale.org/\nDocker tutorial: http://jsta.github.io/r-docker-tutorial/\nEnvironment Management with Docker: https://environments.rstudio.com/docker\nSharing and Running R code using Docker: https://aboland.ie/Docker.html"
  },
  {
    "objectID": "day4-sharing_things.html#xarigan",
    "href": "day4-sharing_things.html#xarigan",
    "title": "Sharing Things",
    "section": "Xarigan",
    "text": "Xarigan\nXarigan is an R package to create slide deck using R Markdown: https://github.com/yihui/xaringan\n\nremotes::install_github('yihui/xaringan')\n\nHere is a good introduction to it: https://www.favstats.eu/post/xaringan_tut/"
  },
  {
    "objectID": "day4-sharing_things.html#quarto-presentations",
    "href": "day4-sharing_things.html#quarto-presentations",
    "title": "Sharing Things",
    "section": "Quarto Presentations",
    "text": "Quarto Presentations\nhttps://quarto.org/docs/presentations/\nhttps://meghan.rbind.io/blog/quarto-slides/"
  },
  {
    "objectID": "day2-remote_server.html",
    "href": "day2-remote_server.html",
    "title": "Working on a remote server",
    "section": "",
    "text": "CERN computing center"
  },
  {
    "objectID": "day2-remote_server.html#learning-objectives",
    "href": "day2-remote_server.html#learning-objectives",
    "title": "Working on a remote server",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this lesson, you will learn:\n\nHow to connect to a remote server\nGet familiar with RStudio server\nGet an introduction to the command line (CLI) & bash"
  },
  {
    "objectID": "day2-remote_server.html#why-working-on-a-remote-machine",
    "href": "day2-remote_server.html#why-working-on-a-remote-machine",
    "title": "Working on a remote server",
    "section": "Why working on a remote machine?",
    "text": "Why working on a remote machine?\nOften the main motivation is to scale your analysis beyond what a personal computer can handle. R being pretty memory intensive, moving to a server often provides you more RAM and thus allows to load larger data in R without the need of slicing your data into chunks. But there are also other advantages, here are the main for scientist:\n\nPower: More CPUs/Cores (24/32/48), More RAM (256/384GB)\nCapacity: More disk space and generally faster storage (in highly optimized RAID arrays)\nSecurity: Data are spread across multiple drives and have nightly backups\nCollaboration: shared folders for code, data, and other materials; same software versions\n\n\n\n\n\n\n\nWarning\n\n\n\n=&gt; The operating system is more likely going to be Linux!!\nMore on this in a few minutes"
  },
  {
    "objectID": "day2-remote_server.html#introduction-to-unix-and-its-siblings",
    "href": "day2-remote_server.html#introduction-to-unix-and-its-siblings",
    "title": "Working on a remote server",
    "section": "Introduction to UNIX and its siblings",
    "text": "Introduction to UNIX and its siblings\n\nUNIX\n\nOriginally developed at AT&T Bell Labs circa 1970. Has experienced a long, multi-branched evolutionary path\n\nPOSIX (Portable Operating System Interface)\n\na set of specifications of what an OS needs to qualify as “a Unix”, to enhance interoperability among all the “Unix” variants\n\n\n\nVarious Unices\n\n\n\nThe unix family tree\n\n\n\n\nOS X\n\nis a Unix!\n\n\n\nLinux\n\nis not fully POSIX-compliant, but certainly can be regarded as functionally Unix\n\n\n\n\n\nSome Unix hallmarks\n\nSupports multi-users, multi-processes\nHighly modular: many small tools that do one thing well, and can be combined\nCulture of text files and streams\nPrimary OS on HPC (High Performance Computing Systems)\nMain OS on which Internet was built"
  },
  {
    "objectID": "day2-remote_server.html#connecting-via-ide---posit-workbench",
    "href": "day2-remote_server.html#connecting-via-ide---posit-workbench",
    "title": "Working on a remote server",
    "section": "Connecting via IDE - Posit Workbench",
    "text": "Connecting via IDE - Posit Workbench\nFrom an user perspective, RStudio Server is your familiar RStudio interface in your web browser. The big difference however is that with RStudio Server the computation will be running on the remote machine instead of your local personal computer. This also means that the files you are seeing through the RStudio Server interface are located on the remote machine. And this also include your R packages!!! This remote file management is the main change you will have to adopt in your workflow.\nTo help with remote files management, the RStudio Server interface as few additional features that we will be discussing in the following sections."
  },
  {
    "objectID": "day2-remote_server.html#connecting-to-meds-analytical-server",
    "href": "day2-remote_server.html#connecting-to-meds-analytical-server",
    "title": "Working on a remote server",
    "section": "Connecting to MEDS Analytical Server",
    "text": "Connecting to MEDS Analytical Server\n\nGot to: https://taylor.bren.ucsb.edu/\nEnter your credentials\nYou are in!\n\n\n\n\n\n\n\n\n\n\n\nClick on the New Session button. You can see that you are able to start both an R (Studio) and jupyter notebook session. Let’s take a few minutes to experiment with the different options.\n\nFor this session, we are going to select the RStudio option and hit Start Session.\n\n\n\n\n\n\n\n\n\nYou should now see a very familiar interface :) Except it is running on the server with a lot of resources at your fingertips!!\n\nFile structure\nLet’s explore explore a little bit the file structure on the server. By default on a Linux server, you are located in the home folder. This folder is only accessible to you and it is where you can store your personal files on a server. You should see 2 folders: R and H\n\n\n\n\n\n\n\n\n\nThe R folder is where your local R packages will be installed, you can ignore it. The H is your H drive that the Bren School is offering to all its students. If you click on it you should see any files you have uploaded there.\nLet us make a folder named github by click on the New Folder button at the top of the tab. We will use this folder (also named directory in linux/unix terms) to clone any GitHub repository.\n\n\nR packages\nIf we go to the Packages tab, we can see a long list of packages that have already be installed by our system administrator (Brad). Those packages have been installed server wide, meaning that all the users have access to them.\n\n\n\n\n\n\n\n\n\nA user can also installed her/his own packages. Let’s try to install the remote package that lets you install R packages directly from GitHub: install.packages(\"remotes\"). Once done, note a new section that appeared on the Packages tab named User Library. Each of us have now its own copy of the package installed (in this R folder we were talking about a few minutes ago).\n\n\n\n\n\n\n\n\n\nA few notes:\n\nIn this example we will have made a better choice to have the remotes package installed once at the system level\nSome R packages depend on external libraries that need to be installed on the server. Those libraries will have to be installed by the system administrator first before you can install the R package\nInstalling an R package on a linux machine generally requires compilation of the code and will thus take more time to install than when you install it from pre-compiled binaries\n\nLook now inside you R folder!!"
  },
  {
    "objectID": "day2-remote_server.html#the-command-line-interface-cli",
    "href": "day2-remote_server.html#the-command-line-interface-cli",
    "title": "Working on a remote server",
    "section": "The Command Line Interface (CLI)",
    "text": "The Command Line Interface (CLI)\nThe CLI provides a direct way to interact with the Operating System, by typing in commands.\n\nWhy the CLI is worth learning\n\nMight be the only interface you have to a High Performance Computer (HPC)\nCommand statements can be reused easily and saved as scripts\nEasier automation and text files manipulation\n\n\n\nA little bit of terminology\n\nCommand Line Interface (CLI): This is a user interface that lets you interact with a computer. It is a legacy from the early days of computers. Now a days computers have graphical user interfaces instead (MacOSX, Windows, Linux, …)\nTerminal: It is a an application that lets you run a command line interface. It used to be a physical machine connected to a mainframe computer\nShell: It is the program that runs the command line. There are many different shells, the most common (an default on most system) being bash (Bourne Again SHell)\n\n\n\n\n\n\n\nThe pitch\n\n\n\nNot convinced? Check this out: the CLI pitch\n\n\n\n\nThe Terminal from RStudio\nYou can access the command line directly from RStudio by using the Terminal tab next to your R console.\n ### Navigating and managing files/directories in *NIX\n\n\npwd: Know where you are\nls: List the content of the directory\ncd: Go inside a directory\n\nSome pseudo directory names. Wherever you are:\n\n~ : Home directory\n. : Here (current directory)\n..: Up one level (upper directory)\n\nLet’s put this into action:\n\ngo to my “Home” directory: cd ~\ngo up one directory level: cd ..\nlist the content: ls\nlist the content showing hidden files: ls -a note that -a is referred as an option (modifies the command)\n\nMore files/directories manipulations:\n\nmkdir: Create a directory\ncp: Copy a file\nmv: Move a file it is also how you rename a file!\nrm / rmdir: Remove a file / directory use those carefully, there is no return / Trash!!\n\nNote: typing is not your thing? the &lt;tab&gt; key is your friend! One hit it will auto-complete the file/directory/path name for you. If there are many options, hit it twice to see the options.\n\n\nPermissions\nAll files have permissions and ownership.\n\n\n\nFile permissions\n\n\n\nList files showing ownership and permissions: ls -l\n\nbrun@taylor:/courses/EDS214$ ls -l\ntotal 16\ndrwxrwxr-x+ 3 brun      esmdomainusers 4096 Aug 20 04:49 data\ndrwxrwxr-x+ 2 katherine esmdomainusers 4096 Aug 18 18:32 example    \nYou can change those permissions:\n\nChange permissions: chmod\nChange ownership: chown\n\n\n\n\n\n\n\nTip\n\n\n\nClear contents in terminal window: clear"
  },
  {
    "objectID": "day2-remote_server.html#general-command-syntax",
    "href": "day2-remote_server.html#general-command-syntax",
    "title": "Working on a remote server",
    "section": "General command syntax",
    "text": "General command syntax\n\ncommand [options] [arguments]\n\nwhere command must be an executable file on your PATH * echo $PATH\nand options can usually take two forms * short form: -a * long form: --all\nYou can combine the options:\nls -ltrh\nWhat do these options do?\nman ls\n\n\n\n\n\n\nTip\n\n\n\nhit spacebar to get to the next page of the manual hit q to exit the help"
  },
  {
    "objectID": "day2-remote_server.html#getting-things-done",
    "href": "day2-remote_server.html#getting-things-done",
    "title": "Working on a remote server",
    "section": "Getting things done",
    "text": "Getting things done\n\nSome useful, special commands using the Control key\n\nCancel (abort) a command: Ctrl-c Note: very different than Windows!!\nStop (suspend) a command: Ctrl-z\nCtrl-z can be used to suspend, then background a process\n\n\n\nProcess management\n\nLike Windows Task Manager, OSX Activity Monitor\ntop, ps, jobs (hit q to get out!)\nkill to delete an unwanted job or process\nForeground and background: &\n\n\n\nWhat about “space”\n\nHow much storage is available on this system? df -h\nHow much storage am “I” using overall? du -hs &lt;folder&gt;\nHow much storage am “I” using, by sub directory? du -h &lt;folder&gt;\nHow much RAM is used? free -h\n\n\n\nExistential questions\n\nWhat is your username? whoami\nWhat is today’s date? date\nWhere are the programs you are using? whereis R also try which -a python\nWhat is the kernel version of your OS: uname -a\nHow long since last reboot: uptime\nWhich shell are you using? echo $0\n\n\n\nHistory\n\nSee your command history: history\nRe-run last command: !!\nRe-run 32th command: !32\nRe-run 5th from last command: !-5\nRe-run last command that started with ‘c’: !c"
  },
  {
    "objectID": "day2-remote_server.html#a-sampling-of-simple-commands-for-dealing-with-files",
    "href": "day2-remote_server.html#a-sampling-of-simple-commands-for-dealing-with-files",
    "title": "Working on a remote server",
    "section": "A sampling of simple commands for dealing with files",
    "text": "A sampling of simple commands for dealing with files\n\nwc count lines, words, and/or characters\ndiff compare two files for differences\nsort sort lines in a file\nuniq report or filter out repeated lines in a file"
  },
  {
    "objectID": "day2-remote_server.html#get-into-the-flow-with-pipes",
    "href": "day2-remote_server.html#get-into-the-flow-with-pipes",
    "title": "Working on a remote server",
    "section": "Get into the flow, with pipes",
    "text": "Get into the flow, with pipes\n\n\n\nstdin, stdout, stderr\n\n\nwc -l *.TXT \nwc -l *.TXT | sort -n\n\n\nnote use of * as character wildcard for zero or more matches (same in Mac and Windows)\n? matches single character;\n\n\nHere the wc -l command, which counts the number of lines in files, is given file names on the command line, so it counts the lines in those files. It writes its output to stdout. sort -n sorts lines in files. It was given no files to sort, so it sorts whatever lines come in via stdin. By piping these together (i.e., by hooking wc’s stdout to sort’s stdin using the pipe operator), the output from wc -l is thereby sorted.\nThere are various operators for redirecting where stdin comes from and where stdout and stderr go:\n\n&lt; file: read stdin from file\n&gt; file: write stdout to file\n2&gt; file: write stderr to file\n&gt;& file: write both stdout and stderr to file\n&gt;&gt; file: append stdout to file\n\nLet’s write the above result to a file:\nwc -l *.TXT | sort -n &gt; csvcount.log\nCaution: except for &gt;&gt;, all forms of &gt; are destructive: Bash overwrites any existing file with an empty file before the program is run.\nWant to get rid of output you don’t want to see? Use the Unix black hole: &gt;& /dev/null. (This is a cultural meme, you’ll see it on T-shirts and license plates.)\nThe above are the main redirections, but there are others.\nTo end a session, simply type exit or logout into the command line\n\n\n\n\n\n\nSetting git on Taylor (if not yet done)\n\n\n\n\n\nAt the Terminal:\n\ngit config --global user.name \"Jane Doe\" \ngit config --global user.email janedoe@example.com\ngit config --global credential.helper 'cache --timeout=10000000'\ngit config --list\n\n\nSetting GitHub token on Taylor\nAt the R Console:\n\n# On your laptop\nusethis::create_github_token() # This should open a web browser on GitHub\n\n# On Taylor \ngitcreds::gitcreds_set()\nusethis::git_sitrep()"
  },
  {
    "objectID": "day2-remote_server.html#cli-practice",
    "href": "day2-remote_server.html#cli-practice",
    "title": "Working on a remote server",
    "section": "CLI Practice",
    "text": "CLI Practice"
  },
  {
    "objectID": "day2-remote_server.html#bonus-vim",
    "href": "day2-remote_server.html#bonus-vim",
    "title": "Working on a remote server",
    "section": "Bonus: vim",
    "text": "Bonus: vim\nThis section is optional depending of our progress and interest. Vim is still the default text editor on many Linux distribution and it is good to know about its basics.\nVim"
  },
  {
    "objectID": "day2-remote_server.html#aknowledgements",
    "href": "day2-remote_server.html#aknowledgements",
    "title": "Working on a remote server",
    "section": "Aknowledgements",
    "text": "Aknowledgements\nThis section reuses materials from NCEAS Open Science for Synthesis (OSS) intensive summer schools and other training. Contributions to this content have been made by Mark Schildhauer, Matt Jones, Jim Regetz and many others; and from EDS-213 10 bash essentials developed by Greg Janée"
  }
]